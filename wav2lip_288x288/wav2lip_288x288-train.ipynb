{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a35e39",
   "metadata": {
    "papermill": {
     "duration": 0.03953,
     "end_time": "2021-08-02T15:03:04.941630",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.902100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wav2Lip\n",
    "**[Wav2Lip](https://arxiv.org/pdf/2008.10010.pdf)** 是一种基于对抗生成网络的由语音驱动的人脸说话视频生成模型。如下图所示，Wav2Lip的网络模型总体上分成三块：生成器、判别器和一个预训练好的Lip-Sync Expert组成。网络的输入有2个：任意的一段视频和一段语音，输出为一段唇音同步的视频。生成器是基于encoder-decoder的网络结构，分别利用2个encoder: speech encoder, identity encoder去对输入的语音和视频人脸进行编码，并将二者的编码结果进行拼接，送入到 face decoder 中进行解码得到输出的视频帧。判别器Visual Quality Discriminator对生成结果的质量进行规范，提高生成视频的清晰度。为了更好的保证生成结果的唇音同步性，Wav2Lip引入了一个预预训练的唇音同步判别模型 Pre-trained Lip-sync Expert，作为衡量生成结果的唇音同步性的额外损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34513e2",
   "metadata": {
    "papermill": {
     "duration": 0.037914,
     "end_time": "2021-08-02T15:03:05.018513",
     "exception": false,
     "start_time": "2021-08-02T15:03:04.980599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Lip-Sync Expert\n",
    "Lip-sync Expert基于 **[SyncNet](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/)**，是一种用来判别语音和视频是否同步的网络模型。如下图所示，SyncNet的输入也是两种：语音特征MFCC和嘴唇的视频帧，利用两个基于卷积神经网络的Encoder分别对输入的语音和视频帧进行降纬和特征提取，将二者的特征都映射到同一个纬度空间中去，最后利用contrastive loss对唇音同步性进行衡量，结果的值越大代表越不同步，结果值越小则代表越同步。在Wav2Lip模型中，进一步改进了SyncNet的网络结构：网络更深；加入了残差网络结构；输入的语音特征被替换成了mel-spectrogram特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461cfed",
   "metadata": {
    "papermill": {
     "duration": 0.07419,
     "end_time": "2021-08-02T15:03:05.158517",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.084327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. 环境的配置\n",
    "- `建议准备一台有显卡的linux系统电脑，或者可以选择使用第三方云服务器` \n",
    "- `Python 3.6 或者更高版本` \n",
    "- ffmpeg: `sudo apt-get install ffmpeg`\n",
    "- 必要的python包的安装，所需要的库名称都已经包含在`requirements.txt`文件中，可以使用 `pip install -r requirements.txt`一次性安装. \n",
    "- 在本实验中利用到了人脸检测的相关技术，需要下载人脸检测预训练模型：Face detection [pre-trained model](https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth) 并移动到 `face_detection/detection/sfd/s3fd.pth`文件夹下. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1514d8",
   "metadata": {
    "papermill": {
     "duration": 0.081877,
     "end_time": "2021-08-02T15:03:05.314858",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.232981",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931aaa3d-6eab-44f9-9cc0-d6e17fe80c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O face_detection/detection/sfd/s3fd.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c15ff",
   "metadata": {
    "papermill": {
     "duration": 0.065854,
     "end_time": "2021-08-02T15:03:05.446911",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.381057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. 数据集的准备及预处理\n",
    "\n",
    "**LRS2 数据集的下载**  \n",
    "实验所需要的数据集下载地址为：<a href=\"http://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html\">LRS2 dataset</a>，下载该数据集需要获得BBC的许可，需要发送申请邮件以获取下载密钥，具体操作详见网页中的指示。下载完成后对数据集进行解压到本目录的`mvlrs_v1/`文件夹下，并将LRS2中的文件列表文件`train.txt, val.txt, test.txt` 移动到`filelists/`文件夹下，最终得到的数据集目录结构如下所示。\n",
    "```\n",
    "data_root (mvlrs_v1)\n",
    "├── main, pretrain (我们只使用main文件夹下的数据)\n",
    "|\t├── 文件夹列表\n",
    "|\t│   ├── 5位以.mp4结尾的视频ID\n",
    "```\n",
    "**数据集预处理**\n",
    "数据集中大多数视频都是包含人的半身或者全身的画面，而我们的模型只需要人脸这一小部分。所以在预处理阶段，我们要对每一个视频进行分帧操作，提取视频的每一帧，之后使用`face detection`工具包对人脸位置进行定位并裁减，只保留人脸的图片帧。同时，我们也需要将每一个视频中的语音分离出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bccdb9-1b21-48d4-a983-6f2add90b315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ../LSR2/demo\n",
    "!mkdir -p ../LSR2/demo\n",
    "!cp -r ../LSR2/main/553* ../LSR2/demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ed106-5da1-4eb6-8507-b43458022c34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python generate_hq_videos.py --data_root ../LSR2/demo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dcfde",
   "metadata": {
    "papermill": {
     "duration": 0.076313,
     "end_time": "2021-08-02T15:03:05.590493",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.514180",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf ../LSR2/lrs2_preprocessed_288x288\n",
    "# !python preprocess.py --data_root \"../LSR2/main_hq\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288\" --batch_size 32 --ngpu 4\n",
    "!rm -rf ../LSR2/lrs2_preprocessed_288x288-demo\n",
    "!python preprocess.py --data_root \"../LSR2/demo\" --preprocessed_root \"../LSR2/lrs2_preprocessed_288x288-demo\" --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e70f29",
   "metadata": {
    "papermill": {
     "duration": 0.057029,
     "end_time": "2021-08-02T15:03:05.717822",
     "exception": false,
     "start_time": "2021-08-02T15:03:05.660793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "预处理后的`lrs2_preprocessed/`文件夹下的目录结构如下\n",
    "```\n",
    "preprocessed_root (lrs2_preprocessed)\n",
    "├── 文件夹列表\n",
    "|\t├── 五位的视频ID\n",
    "|\t│   ├── *.jpg\n",
    "|\t│   ├── audio.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5bb63-382e-4670-bd8e-aa6d7702c34d",
   "metadata": {},
   "source": [
    "获取对应的文件列表并更新到filelists/train.txt和filelists/eval.txt。只保存对应的视频名称即可。代码可以参考，对视频样本重命名并生成对应的命名列表，此处视频文件数量过少<2，会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7921c0-363d-41e9-9533-7ab4715f97b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python generate_filelists.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6d2a7-7f3e-40ea-96c4-996aaee42a32",
   "metadata": {},
   "source": [
    "Training the expert discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0bb64-29af-486d-b14e-5eecf0be8c17",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo/ --checkpoint_dir ./savedmodel \n",
    "# !python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel \n",
    "# !python color_syncnet_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel --checkpoint_path ./savedmodel/checkpoint_step000032000.pth \n",
    "# --checkpoint_path ./checkpoints/lipsync_expert.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3dc53-722f-428e-8305-7b9d42ae082a",
   "metadata": {},
   "source": [
    "执行如下命令，开始训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b989d6e-be14-47fc-9f10-2226f02bc4c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000000001.pth \n",
    "# !python wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288 --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000032000.pth \n",
    "# --checkpoint_path ./checkpoints/wav2lip.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764fda7-3c76-45d8-be8f-391ad409a361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000000001.pth \n",
    "# !python hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288 --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000032000.pth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04656c-3efc-4ddc-a4db-1f328b5cf457",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python wloss_hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288-demo/ --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000000001.pth\n",
    "# !python wloss_hq_wav2lip_train.py --data_root ../LSR2/lrs2_preprocessed_288x288/ --checkpoint_dir ./savedmodel --syncnet_checkpoint_path ./savedmodel/checkpoint_step000050000.pth \n",
    "# --checkpoint_path ./checkpoints/wav2lip.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8d8df-60a8-42e7-8827-b7eabf39cc42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!python inference.py --checkpoint_path ./savedmodel/wav2lip_checkpoint_step000093000.pth --face ../videos/97.mp4 --audio ../videos/test.wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b59cc8-333d-4f88-99ae-1120804ebf93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30478.041156,
   "end_time": "2021-08-02T23:30:53.753624",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-02T15:02:55.712468",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
