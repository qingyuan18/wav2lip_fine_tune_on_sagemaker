{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4416c96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An sample to finetune wave2lip on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95febd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e94ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0c829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## download training script from github\n",
    "!rm -rf ./wav2lip_288x288\n",
    "!git clone https://github.com/whn09/wav2lip_288x288.git\n",
    "!cp ./s5cmd ./wav2lip_288x288/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5a450",
   "metadata": {},
   "source": [
    "## Download pretrained model(expert Discriminator & face detect) and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e0bed-ee4f-43d5-afbb-c61907353552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ./wav2lip_288x288/face_detection/detection/sfd/ && wget https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
    "!mv ./wav2lip_288x288/face_detection/detection/sfd/s3fd-619a316812.pth ./wav2lip_288x288/face_detection/detection/sfd/s3fd.pth\n",
    "# !cd ./Wav2Lip/models && wget https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d498",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2057f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN  apt-get update\n",
    "RUN  echo \"Y\"|apt-get install ffmpeg\n",
    "RUN  pip3 install deepspeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ee553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b0fb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53800617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-wav2lip-288x288-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d98c7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa7288-4f61-4896-93b7-12fd03055c53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef55e2",
   "metadata": {},
   "source": [
    "use s3 dataset path, which is aligned with data_root of wav2lip \n",
    "should be like:  \n",
    "data_root (mvlrs_v1)  \n",
    "├── main, pretrain (we use only main folder in this work)  \n",
    "|\t├── list of folders  \n",
    "|\t│   ├── five-digit numbered video IDs ending with (.mp4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90becfa3-2ec1-4735-b337-33891f12d1d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://bj.bcebos.com/ai-studio-online/88f38e14dc9f4893bdb3ad6857b810a9e0ed6d63f4f84d7da0b69e165ca4f7a5?authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2022-09-04T15%3A27%3A21Z%2F-1%2F%2F2048bfe72ab62c84e2a6622f565ac7cc30830ffdace4607bf847909d1038b5b0&responseContentDisposition=attachment%3B%20filename%3Dmain.zip\n",
    "!./s5cmd sync ./main2/ s3://{sagemaker_default_bucket}/wav2lip_288x288/train_data/main2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7f140-733f-4626-97af-78b3436c8616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 准备filelists/train.txt\n",
    "import os\n",
    "\n",
    "import time\n",
    "from glob import glob\n",
    "import shutil,os\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# 去除名字的特殊符号，统一序号视频文件命名\n",
    " \n",
    "# def original_video_name_format():\n",
    "#     base_path = \"../LSR2/main\"\n",
    "#     result = list(glob(\"{}/*\".format(base_path),recursive=False))\n",
    "#     file_num = 0\n",
    "#     result_list = []\n",
    " \n",
    "#     for each in result:\n",
    "#         file_num +=1\n",
    "#         new_position =\"{0}{1}\".format( int(time.time()),file_num)\n",
    "#         result_list.append(new_position)\n",
    "#         shutil.move(each, os.path.join(base_path,new_position+\".mp4\"))\n",
    "#         pass\n",
    "\n",
    "def trained_data_name_format():\n",
    "    base_path = \"../LSR2/lrs2_preprocessed\"\n",
    "    # result = list(glob(\"{}/*\".format(base_path)))\n",
    "    result = os.listdir(base_path)\n",
    "    print(result)\n",
    "    result_list = []\n",
    "    for i,dirpath in enumerate(result):\n",
    "        # shutil.move(dirpath,\"{0}/{1}\".format(base_path,i))\n",
    "        # result_list.append(str(i))\n",
    "        # print('dirpath:', dirpath)\n",
    "        result_list.append(dirpath)\n",
    "    if len(result_list)<14:\n",
    "        test_result=val_result=train_result=result_list\n",
    "    else:\n",
    "        train_result,test_result = train_test_split(result_list,test_size=0.15, random_state=42)\n",
    "        test_result, val_result = train_test_split(test_result, test_size=0.5, random_state=42)\n",
    " \n",
    "    for file_name,dataset in zip((\"train.txt\",\"test.txt\",\"val.txt\"),(train_result,test_result,val_result)):\n",
    "        with open(os.path.join(\"filelists\",file_name),'w',encoding='utf-8') as fi:\n",
    "            for dataset_i in dataset:\n",
    "                # print('dataset_i:', dataset_i)\n",
    "                video_result = os.listdir(os.path.join(base_path, dirpath))\n",
    "                # print('video_result:', video_result)\n",
    "                video_result = [dataset_i+'/'+video for video in video_result]\n",
    "                fi.write(\"\\n\".join(video_result))\n",
    "                fi.write(\"\\n\")\n",
    " \n",
    "    # print(\"\\n\".join(result_list))\n",
    "\n",
    "trained_data_name_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8ac60-b8bd-44b6-9308-f024ea49c040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### for local data process test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f553a9-582a-43a5-9dd9-6742c390c02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a618b1d-5bf1-492a-92e0-a9043c117da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###data process\n",
    "#!pip install -r ./requirements.txt\n",
    "!cd ./Wav2Lip/ && python preprocess.py --data_root ../main2 --preprocessed_root ../lrs2_preprocessed2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62c4f7-5beb-4c14-be0c-29ed079bbd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./s5cmd sync ./lrs2_preprocessed2/ s3://{sagemaker_default_bucket}/288x288/train_data/lrs2_preprocessed/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d5b8f-188e-4e46-b998-903b462f59cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 开始训练(单机单卡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55cedcb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://${sagemaker_default_bucket}/wav2lip_288x288/train_data/main2/* /tmp/main2/\n",
    "pip install -r ./requirements.txt\n",
    "\n",
    "echo \"--------preprocess wav-----------\"\n",
    "python ./preprocess.py --data_root /tmp/main2 --preprocessed_root /tmp/lrs2_preprocessed2/\n",
    "echo \"finished preprocess\"\n",
    "\n",
    "echo \"--------prepare tain list--------\"\n",
    "python ./generate_filelists.py\n",
    "echo \"finished train list prepare\"\n",
    "\n",
    "echo \"--------train the expert discriminator------\"\n",
    "startdt=$(date +%s)\n",
    "python ./color_syncnet_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_syncnet/\n",
    "enddt=$(date +%s)\n",
    "interval_minutes=$(( (enddt - startdt) ))\n",
    "echo \"时间间隔为 $interval_minutes 秒\"\n",
    "echo \"finished train syncnet\"\n",
    "\n",
    "\n",
    "echo \"--------train wav2lip-----------------------\" \n",
    "python ./hq_wav2lip_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_wav2lip_288x288/ --syncnet_checkpoint_path /tmp/trained_syncnet/checkpoint_step000000001.pth\n",
    "echo \"finished wav2lip\"\n",
    "\n",
    "./s5cmd sync /tmp/trained_wav2lip_288x288/ s3://${sagemaker_default_bucket}/models/wav2lip_288x288/output/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "\n",
    "###inference\n",
    "#echo \"begin inference\"\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/face_video/* /tmp/face_video/\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/audio/* /tmp/audio/\n",
    "#python ./inference.py --checkpoint_path /tmp/trained_wav2lip_288x288/checkpoint_step000000001.pth  --face /tmp/face_video/VID20230623143819.mp4 --audio /tmp/audio/测试wav2lip.mp3\n",
    "#./s5cmd sync ./results/result_voice.mp4  s3://${sagemaker_default_bucket}/models/wav2lip_288x288/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "517889b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-wav2lip-288x288-demo:latest'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad795754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/wav2lip_288x288/train_data/'.format(sagemaker_default_bucket)\n",
    "inputs = {'data_root': train_data_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab36100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'sagemaker_default_bucket': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'wav2lip-288x288-demo'         \n",
    "\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train.sh',\n",
    "                      source_dir='./wav2lip_288x288/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      keep_alive_period_in_seconds=3600,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa5918-57f2-455e-98f3-ecb59c47135f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://${sagemaker_default_bucket}/models/wav2lip_288x288/results/result_voice.mp4 ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d6593-90ad-4409-ae65-17e618a6c91c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### deepspeed （单机多卡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420efdac-3199-40e2-a81a-1c727ca7c868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-wav2lip-288x288-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e5783-8d3e-4964-bcdd-bc3b96fe8c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960df78-8a29-4991-955d-5c04c82dd1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/wav2lip_288x288/train_data/'.format(sagemaker_default_bucket)\n",
    "inputs = {'data_root': train_data_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21eb734a-d22b-47a1-886a-c8a592b0a53d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/color_syncnet_dist_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/color_syncnet_dist_train.py\n",
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "import audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import deepspeed\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the expert lip-sync discriminator')\n",
    "\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True)\n",
    "\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\n",
    "parser.add_argument('--checkpoint_path', help='Resumed from this checkpoint', default=None, type=str)\n",
    "## Include DeepSpeed configuration arguments\n",
    "parser.add_argument('--local_rank', type=int, default=-1, help='local rank passed from distributed launcher')\n",
    "parser = deepspeed.add_config_arguments(parser)\n",
    "args = parser.parse_args()\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(args.data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))  # '{}.jpg' '{}.png'\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        # num_frames = (T x hop_size * fps) / sample_rate\n",
    "        start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))  # '*.jpg' '*.png'\n",
    "            # print('vidname:', vidname, 'img_names:', len(img_names))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                # print('CONTINUE: len(img_names) <= 3 * syncnet_T')\n",
    "                continue\n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            if random.choice([True, False]):\n",
    "                y = torch.ones(1).float()\n",
    "                chosen = img_name\n",
    "            else:\n",
    "                y = torch.zeros(1).float()\n",
    "                chosen = wrong_img_name\n",
    "\n",
    "            window_fnames = self.get_window(chosen)\n",
    "            if window_fnames is None:\n",
    "                # print('CONTINUE: window_fnames is None. chosen:', chosen)\n",
    "                continue\n",
    "\n",
    "            window = []\n",
    "            all_read = True\n",
    "            for fname in window_fnames:\n",
    "                img = cv2.imread(fname)\n",
    "                if img is None:\n",
    "                    all_read = False\n",
    "                    break\n",
    "                try:\n",
    "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "                except Exception as e:\n",
    "                    all_read = False\n",
    "                    break\n",
    "\n",
    "                window.append(img)\n",
    "\n",
    "            if not all_read: continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")  # \"audio.wav\" \"../audio.wav\"\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                # print('CONTINUE Exception:', e, ', wavpath:', wavpath, ', wav:', wav)\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                # print('CONTINUE: mel.shape[0] != syncnet_mel_step_size')\n",
    "                continue\n",
    "\n",
    "            # H x W x 3 * T\n",
    "            x = np.concatenate(window, axis=2) / 255.\n",
    "            x = x.transpose(2, 0, 1)\n",
    "            x = x[:, x.shape[1]//2:]\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "            \n",
    "            # print('x, mel, y:', x, mel, y)\n",
    "\n",
    "            return x, mel, y\n",
    "\n",
    "logloss = nn.BCELoss()\n",
    "def cosine_loss(a, v, y):\n",
    "    # print('a:', a.shape, ', v:', v.shape, ', y:', y.shape)\n",
    "    # print('a:', torch.isnan(a).any(), ', v:', torch.isnan(v).any(), ', y:', torch.isnan(y).any())\n",
    "    # print('a:', torch.isinf(a).any(), ', v:', torch.isinf(v).any(), ', y:', torch.isinf(y).any())\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    # print('d:', d.shape, torch.isnan(d).any(), d)\n",
    "    # print('y:', y.shape, torch.isnan(y).any(), y)\n",
    "    d = torch.clamp(d, min=0.0)  # TODO Pytorch.clamp：将小于0的元素修改为0，截断元素的取值空间\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "    # print('loss:', loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train(device, model_engine, train_data_loader, test_data_loader, optimizer_deepspeed,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "    \n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, mel, y) in prog_bar:\n",
    "        # for step, (x, mel, y) in enumerate(train_data_loader):\n",
    "            model_engine.train()\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            # print('x:', x.shape)\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model_engine(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            model_engine.backward(loss)\n",
    "            model_engine.step()\n",
    "\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model_engine, optimizer_deepspeed, global_step, checkpoint_dir, global_epoch)\n",
    "\n",
    "            #if global_step % hparams.syncnet_eval_interval == 0:\n",
    "            #    with torch.no_grad():\n",
    "            #        eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
    "\n",
    "            prog_bar.set_description('Loss: {}'.format(running_loss / (step + 1)))\n",
    "            # print('Loss: {}'.format(running_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
    "    eval_steps = 1400\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    losses = []\n",
    "    while 1:\n",
    "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        averaged_loss = sum(losses) / len(losses)\n",
    "        print(averaged_loss)\n",
    "\n",
    "        return\n",
    "\n",
    "def save_checkpoint(model_engine, optimizer, step, checkpoint_dir, epoch):\n",
    "    # 获取当前时间戳\n",
    "    timestamp = int(time.time())\n",
    "    model_engine.save_checkpoint(checkpoint_dir)\n",
    "    #torch_model = model_engine.module\n",
    "    #output_model_path = checkpoint_dir+str(timestamp)+\"_\"+str(step)+\"_trained_syncnet.pth\"\n",
    "    #torch.save(torch_model.state_dict(), output_model_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_dir)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "    checkpoint_path = args.checkpoint_path\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "\n",
    "    # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "    print('train_dataset:', len(train_dataset))\n",
    "    print('test_dataset:', len(test_dataset))\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.syncnet_batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.syncnet_batch_size,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # Model\n",
    "    model = SyncNet().to(device)\n",
    "    print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.syncnet_lr)\n",
    "\n",
    "    ###deepspeed 改造#################\n",
    "    model_engine, optimizer_deepspeed, _, _ = deepspeed.initialize(args=args,model=model,optimizer=optimizer)\n",
    "    \n",
    "    if checkpoint_path is not None:\n",
    "        load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "\n",
    "    train(device, model_engine, train_data_loader, test_data_loader, optimizer_deepspeed,\n",
    "          checkpoint_dir=checkpoint_dir,\n",
    "          checkpoint_interval=hparams.syncnet_checkpoint_interval,\n",
    "          nepochs=hparams.nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cea58ef0-542a-4a5d-82fa-83c5ed0b3c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/hq_wav2lip_train_deepspeed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/hq_wav2lip_train_deepspeed.py\n",
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip, Wav2Lip_disc_qual\n",
    "import audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the Wav2Lip model WITH the visual quality discriminator')\n",
    "\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True, type=str)\n",
    "\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\n",
    "parser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True, type=str)\n",
    "\n",
    "parser.add_argument('--checkpoint_path', help='Resume generator from this checkpoint', default=None, type=str)\n",
    "parser.add_argument('--disc_checkpoint_path', help='Resume quality disc from this checkpoint', default=None, type=str)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(args.data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def read_window(self, window_fnames):\n",
    "        if window_fnames is None: return None\n",
    "        window = []\n",
    "        for fname in window_fnames:\n",
    "            img = cv2.imread(fname)\n",
    "            if img is None:\n",
    "                return None\n",
    "            try:\n",
    "                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "            window.append(img)\n",
    "\n",
    "        return window\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        if type(start_frame) == int:\n",
    "            start_frame_num = start_frame\n",
    "        else:\n",
    "            start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "        \n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "    def get_segmented_mels(self, spec, start_frame):\n",
    "        mels = []\n",
    "        assert syncnet_T == 5\n",
    "        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n",
    "        if start_frame_num - 2 < 0: return None\n",
    "        for i in range(start_frame_num, start_frame_num + syncnet_T):\n",
    "            m = self.crop_audio_window(spec, i - 2)\n",
    "            if m.shape[0] != syncnet_mel_step_size:\n",
    "                return None\n",
    "            mels.append(m.T)\n",
    "\n",
    "        mels = np.asarray(mels)\n",
    "\n",
    "        return mels\n",
    "\n",
    "    def prepare_window(self, window):\n",
    "        # 3 x T x H x W\n",
    "        x = np.asarray(window) / 255.\n",
    "        x = np.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            \n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            window_fnames = self.get_window(img_name)\n",
    "            wrong_window_fnames = self.get_window(wrong_img_name)\n",
    "            if window_fnames is None or wrong_window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = self.read_window(window_fnames)\n",
    "            if window is None:\n",
    "                continue\n",
    "\n",
    "            wrong_window = self.read_window(wrong_window_fnames)\n",
    "            if wrong_window is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "            \n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n",
    "            if indiv_mels is None: continue\n",
    "\n",
    "            window = self.prepare_window(window)\n",
    "            y = window.copy()\n",
    "            window[:, :, window.shape[2]//2:] = 0.\n",
    "\n",
    "            wrong_window = self.prepare_window(wrong_window)\n",
    "            x = np.concatenate([window, wrong_window], axis=0)\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n",
    "            y = torch.FloatTensor(y)\n",
    "            return x, indiv_mels, mel, y\n",
    "\n",
    "def save_sample_images(x, g, gt, global_step, checkpoint_dir):\n",
    "    x = (x.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255.).astype(np.uint8)\n",
    "    g = (g.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255.).astype(np.uint8)\n",
    "    gt = (gt.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255.).astype(np.uint8)\n",
    "\n",
    "    refs, inps = x[..., 3:], x[..., :3]\n",
    "    folder = join(checkpoint_dir, \"samples_step{:09d}\".format(global_step))\n",
    "    if not os.path.exists(folder): os.mkdir(folder)\n",
    "    collage = np.concatenate((refs, inps, g, gt), axis=-2)\n",
    "    for batch_idx, c in enumerate(collage):\n",
    "        for t in range(len(c)):\n",
    "            cv2.imwrite('{}/{}_{}.jpg'.format(folder, batch_idx, t), c[t])\n",
    "\n",
    "logloss = nn.BCELoss()\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "syncnet = SyncNet().to(device)\n",
    "for p in syncnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "recon_loss = nn.L1Loss()\n",
    "def get_sync_loss(mel, g):\n",
    "    g = g[:, :, :, g.size(3)//2:]\n",
    "    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n",
    "    # B, 3 * T, H//2, W\n",
    "    a, v = syncnet(mel, g)\n",
    "    y = torch.ones(g.size(0), 1).float().to(device)\n",
    "    return cosine_loss(a, v, y)\n",
    "\n",
    "def train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "\n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            disc.train()\n",
    "            model.train()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            ### Train generator now. Remove ALL grads. \n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### Remove all gradients before Training disc\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "            disc_real_loss.backward()\n",
    "\n",
    "            pred = disc(g.detach())\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "            disc_fake_loss.backward()\n",
    "\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            running_disc_real_loss += disc_real_loss.item()\n",
    "            running_disc_fake_loss += disc_fake_loss.item()\n",
    "\n",
    "            if global_step % checkpoint_interval == 0:\n",
    "                save_sample_images(x, g, gt, global_step, checkpoint_dir)\n",
    "\n",
    "            # Logs\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "\n",
    "            running_l1_loss += l1loss.item()\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                running_sync_loss += sync_loss.item()\n",
    "            else:\n",
    "                running_sync_loss += 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss += perceptual_loss.item()\n",
    "            else:\n",
    "                running_perceptual_loss += 0.\n",
    "\n",
    "            if global_step==1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n",
    "\n",
    "\n",
    "            #if global_step % hparams.eval_interval == 0:\n",
    "            #    with torch.no_grad():\n",
    "            #        average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n",
    "\n",
    "            #        if average_sync_loss < .75:\n",
    "            #            hparams.set_hparam('syncnet_wt', 0.03)\n",
    "\n",
    "            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n",
    "                                                                                        running_sync_loss / (step + 1),\n",
    "                                                                                        running_perceptual_loss / (step + 1),\n",
    "                                                                                        running_disc_fake_loss / (step + 1),\n",
    "                                                                                        running_disc_real_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, disc):\n",
    "    eval_steps = 300\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n",
    "    while 1:\n",
    "        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n",
    "            model.eval()\n",
    "            disc.eval()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "            pred = disc(g)\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "\n",
    "            running_disc_real_loss.append(disc_real_loss.item())\n",
    "            running_disc_fake_loss.append(disc_fake_loss.item())\n",
    "\n",
    "            sync_loss = get_sync_loss(mel, g)\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            running_l1_loss.append(l1loss.item())\n",
    "            running_sync_loss.append(sync_loss.item())\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss.append(perceptual_loss.item())\n",
    "            else:\n",
    "                running_perceptual_loss.append(0.)\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n",
    "                                                            sum(running_sync_loss) / len(running_sync_loss),\n",
    "                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n",
    "                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n",
    "                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n",
    "        return sum(running_sync_loss) / len(running_sync_loss)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint_syncnet(path, model):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"module\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "\n",
    "    # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.batch_size,\n",
    "        num_workers=4)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "     # Model\n",
    "    model = Wav2Lip().to(device)\n",
    "    disc = Wav2Lip_disc_qual().to(device)\n",
    "\n",
    "    print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.initial_learning_rate, betas=(0.5, 0.999))\n",
    "    disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n",
    "                           lr=hparams.disc_initial_learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "\n",
    "    if args.disc_checkpoint_path is not None:\n",
    "        load_checkpoint(args.disc_checkpoint_path, disc, disc_optimizer, \n",
    "                                reset_optimizer=False, overwrite_global_states=False)\n",
    "        \n",
    "    load_checkpoint_syncnet(args.syncnet_checkpoint_path, syncnet)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    # Train!\n",
    "    train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21b9cfe0-ad7c-4fe7-8e81-3a0389d2cc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/ds_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/ds_config.json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": false,\n",
    "    \"auto_cast\": true,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"betas\": [\n",
    "        0.8,\n",
    "        0.999\n",
    "      ],\n",
    "      \"eps\": 1e-8,\n",
    "      \"weight_decay\": 3e-7\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "      \"type\": \"WarmupLR\",\n",
    "      \"params\": {\n",
    "          \"warmup_min_lr\": 0,\n",
    "          \"warmup_max_lr\": 0.001,\n",
    "          \"warmup_num_steps\": 1000\n",
    "      }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "      \"stage\": 0\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": 2,\n",
    "  \"gradient_clipping\": 0.0,\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": 64,\n",
    "  \"train_micro_batch_size_per_gpu\": 4,\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3436f11c-46b2-4586-8fec-bed0e7b37d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/train-distribute.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/train-distribute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://${MODEL_S3_BUCKET}/wav2lip_288x288/train_data/main2/* /tmp/main2/\n",
    "pip install -r ./requirements.txt\n",
    "\n",
    "echo \"--------preprocess wav-----------\"\n",
    "python ./preprocess.py --data_root /tmp/main2 --preprocessed_root /tmp/lrs2_preprocessed2/\n",
    "echo \"finished preprocess\"\n",
    "\n",
    "echo \"--------prepare tain list--------\"\n",
    "python ./generate_filelists.py\n",
    "echo \"finished train list prepare\"\n",
    "\n",
    "echo \"--------train the expert discriminator------\"\n",
    "startdt=$(date +%s)\n",
    "deepspeed --num_gpus=8 ./color_syncnet_dist_train.py \\\n",
    "          --data_root /tmp/lrs2_preprocessed2/ \\\n",
    "          --checkpoint_dir /tmp/trained_syncnet/  \\\n",
    "          --deepspeed --deepspeed_config ds_config.json\n",
    "enddt=$(date +%s)\n",
    "interval_minutes=$(( (enddt - startdt) ))\n",
    "echo \"时间间隔为 $interval_minutes 秒\"\n",
    "echo \"finished train syncnet\"\n",
    "\n",
    "trained_syncnet_model_file=$(find /tmp/trained_syncnet/ -name  \"*.pt\" -print)\n",
    "trained_syncnet_model_file=$(echo $trained_syncnet_model_file|cut -d' ' -f1)\n",
    "echo \"trained_syncnet_model_file: \"${trained_syncnet_model_file}\n",
    "    \n",
    "echo \"--------train wav2lip-----------------------\" \n",
    "python ./hq_wav2lip_train_deepspeed.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_wav2lip_288x288/ --syncnet_checkpoint_path $trained_syncnet_model_file\n",
    "#python ./hq_wav2lip_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_wav2lip_288x288/ --syncnet_checkpoint_path /tmp/trained_syncnet/10/global_step2/mp_rank_00_model_states.pt\n",
    "          \n",
    "echo \"finished wav2lip\"\n",
    "\n",
    "./s5cmd sync /tmp/trained_wav2lip_288x288/ s3://$MODEL_S3_BUCKET/wav2lip_288x288/output/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "\n",
    "###inference\n",
    "#echo \"begin inference\"\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/face_video/* /tmp/face_video/\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/audio/* /tmp/audio/\n",
    "#python ./inference.py --checkpoint_path /tmp/trained_wav2lip_288x288/checkpoint_step000000001.pth  --face /tmp/face_video/VID20230623143819.mp4 --audio /tmp/audio/测试wav2lip.mp3\n",
    "#./s5cmd sync ./results/result_voice.mp4  s3://${sagemaker_default_bucket}/models/wav2lip_288x288/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6372dad3-4388-4925-91c4-a44b2f95bca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: wav2lip-288x288-demo-2023-10-21-08-08-21-699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-21 08:10:46 Starting - Starting the training job......\n",
      "2023-10-21 08:11:31 Starting - Preparing the instances for training.........\n",
      "2023-10-21 08:13:10 Downloading - Downloading input data...\n",
      "2023-10-21 08:13:30 Training - Downloading the training image.....................\n",
      "2023-10-21 08:17:06 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:00,953 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:01,013 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:01,024 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:01,026 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:02,475 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:02,550 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:02,623 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:02,634 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data_root\": \"/opt/ml/input/data/data_root\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data_root\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.48xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"wav2lip-288x288-demo-2023-10-21-08-08-21-699\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/wav2lip-288x288-demo-2023-10-21-08-08-21-699/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-distribute.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 192,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.48xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.48xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-distribute.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-distribute.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"data_root\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"data_root\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.48xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-distribute.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=192\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/wav2lip-288x288-demo-2023-10-21-08-08-21-699/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"data_root\":\"/opt/ml/input/data/data_root\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.48xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data_root\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"wav2lip-288x288-demo-2023-10-21-08-08-21-699\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/wav2lip-288x288-demo-2023-10-21-08-08-21-699/source/sourcedir.tar.gz\",\"module_name\":\"train-distribute.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":192,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.48xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.48xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-distribute.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATA_ROOT=/opt/ml/input/data/data_root\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train-distribute.sh \"\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:04.312: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:04,316 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-21 08:18:04,337 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/.ipynb_checkpoints/00001-checkpoint.txt /tmp/main2/5536876846942893978/.ipynb_checkpoints/00001-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/.ipynb_checkpoints/00041-checkpoint.txt /tmp/main2/5536876846942893978/.ipynb_checkpoints/00041-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/.ipynb_checkpoints/00056-checkpoint.txt /tmp/main2/5536876846942893978/.ipynb_checkpoints/00056-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00001.txt /tmp/main2/5536876846942893978/00001.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00007.txt /tmp/main2/5536876846942893978/00007.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00018.txt /tmp/main2/5536876846942893978/00018.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/.ipynb_checkpoints/00009-checkpoint.txt /tmp/main2/5536876846942893978/.ipynb_checkpoints/00009-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00030.txt /tmp/main2/5536876846942893978/00030.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00036.txt /tmp/main2/5536876846942893978/00036.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00012.txt /tmp/main2/5536876846942893978/00012.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00009.txt /tmp/main2/5536876846942893978/00009.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00011.txt /tmp/main2/5536876846942893978/00011.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00032.txt /tmp/main2/5536876846942893978/00032.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00010.txt /tmp/main2/5536876846942893978/00010.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/.ipynb_checkpoints/00059-checkpoint.txt /tmp/main2/5536876846942893978/.ipynb_checkpoints/00059-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00014.mp4 /tmp/main2/5536876846942893978/00014.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00024.mp4 /tmp/main2/5536876846942893978/00024.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00002.mp4 /tmp/main2/5536876846942893978/00002.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00001.mp4 /tmp/main2/5536876846942893978/00001.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00009.mp4 /tmp/main2/5536876846942893978/00009.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00041.mp4 /tmp/main2/5536876846942893978/00041.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00002.txt /tmp/main2/5536876846942893978/00002.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00014.txt /tmp/main2/5536876846942893978/00014.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00015.txt /tmp/main2/5536876846942893978/00015.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00013.txt /tmp/main2/5536876846942893978/00013.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00036.mp4 /tmp/main2/5536876846942893978/00036.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00006.txt /tmp/main2/5536876846942893978/00006.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00019.txt /tmp/main2/5536876846942893978/00019.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00024.txt /tmp/main2/5536876846942893978/00024.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00048.txt /tmp/main2/5536876846942893978/00048.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00026.txt /tmp/main2/5536876846942893978/00026.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00040.txt /tmp/main2/5536876846942893978/00040.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00012.txt /tmp/main2/5540197286159433545/00012.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00068.txt /tmp/main2/5536876846942893978/00068.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00013.txt /tmp/main2/5540197286159433545/00013.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00007.txt /tmp/main2/5540197286159433545/00007.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00035.txt /tmp/main2/5536876846942893978/00035.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00001.txt /tmp/main2/5540197286159433545/00001.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/.ipynb_checkpoints/00007-checkpoint.txt /tmp/main2/5540197286159433545/.ipynb_checkpoints/00007-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00059.txt /tmp/main2/5536876846942893978/00059.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00044.txt /tmp/main2/5536876846942893978/00044.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00065.txt /tmp/main2/5536876846942893978/00065.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00060.txt /tmp/main2/5536876846942893978/00060.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00070.txt /tmp/main2/5536876846942893978/00070.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00073.txt /tmp/main2/5536876846942893978/00073.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00058.txt /tmp/main2/5536876846942893978/00058.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/.ipynb_checkpoints/00001-checkpoint.txt /tmp/main2/5540197286159433545/.ipynb_checkpoints/00001-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00041.txt /tmp/main2/5536876846942893978/00041.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/.ipynb_checkpoints/00012-checkpoint.txt /tmp/main2/5540197286159433545/.ipynb_checkpoints/00012-checkpoint.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00056.txt /tmp/main2/5536876846942893978/00056.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00034.txt /tmp/main2/5536876846942893978/00034.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00072.txt /tmp/main2/5536876846942893978/00072.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00065.mp4 /tmp/main2/5536876846942893978/00065.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00060.mp4 /tmp/main2/5536876846942893978/00060.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00019.mp4 /tmp/main2/5536876846942893978/00019.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00026.mp4 /tmp/main2/5536876846942893978/00026.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00035.mp4 /tmp/main2/5536876846942893978/00035.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00044.mp4 /tmp/main2/5536876846942893978/00044.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00010.mp4 /tmp/main2/5536876846942893978/00010.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00013.mp4 /tmp/main2/5536876846942893978/00013.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00012.mp4 /tmp/main2/5536876846942893978/00012.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00015.mp4 /tmp/main2/5536876846942893978/00015.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00034.mp4 /tmp/main2/5536876846942893978/00034.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00048.mp4 /tmp/main2/5536876846942893978/00048.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00073.mp4 /tmp/main2/5536876846942893978/00073.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00011.mp4 /tmp/main2/5536876846942893978/00011.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00001.mp4 /tmp/main2/5540197286159433545/00001.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00056.mp4 /tmp/main2/5536876846942893978/00056.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00018.mp4 /tmp/main2/5536876846942893978/00018.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00012.mp4 /tmp/main2/5540197286159433545/00012.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00040.mp4 /tmp/main2/5536876846942893978/00040.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00072.mp4 /tmp/main2/5536876846942893978/00072.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00013.mp4 /tmp/main2/5540197286159433545/00013.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00058.mp4 /tmp/main2/5536876846942893978/00058.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5540197286159433545/00007.mp4 /tmp/main2/5540197286159433545/00007.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00070.mp4 /tmp/main2/5536876846942893978/00070.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00068.mp4 /tmp/main2/5536876846942893978/00068.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00007.mp4 /tmp/main2/5536876846942893978/00007.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00059.mp4 /tmp/main2/5536876846942893978/00059.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00006.mp4 /tmp/main2/5536876846942893978/00006.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00032.mp4 /tmp/main2/5536876846942893978/00032.mp4\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/train_data/main2/5536876846942893978/00030.mp4 /tmp/main2/5536876846942893978/00030.mp4\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: librosa>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 1)) (0.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.1 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting opencv-contrib-python>=4.2.0.34\u001b[0m\n",
      "\u001b[34mDownloading opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.8/67.8 MB 37.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python>=4.1.0.25 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 4)) (4.7.0.68)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 5)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 6)) (0.14.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.45.0 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 7)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba>=0.48 in /opt/conda/lib/python3.9/site-packages (from -r ./requirements.txt (line 8)) (0.56.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soundfile>=0.10.2 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: resampy>=0.2.2 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.0.10 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pooch>=1.0 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.9/site-packages (from librosa>=0.7.0->-r ./requirements.txt (line 1)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.1.0->-r ./requirements.txt (line 5)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision>=0.3.0->-r ./requirements.txt (line 6)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchvision>=0.3.0->-r ./requirements.txt (line 6)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.9/site-packages (from numba>=0.48->-r ./requirements.txt (line 8)) (0.39.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from numba>=0.48->-r ./requirements.txt (line 8)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.9/site-packages (from pooch>=1.0->librosa>=0.7.0->-r ./requirements.txt (line 1)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision>=0.3.0->-r ./requirements.txt (line 6)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision>=0.3.0->-r ./requirements.txt (line 6)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision>=0.3.0->-r ./requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->torchvision>=0.3.0->-r ./requirements.txt (line 6)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.19.1->librosa>=0.7.0->-r ./requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa>=0.7.0->-r ./requirements.txt (line 1)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.7.0->-r ./requirements.txt (line 1)) (2.21)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: opencv-contrib-python\u001b[0m\n",
      "\u001b[34mSuccessfully installed opencv-contrib-python-4.8.1.78\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m--------preprocess wav-----------\u001b[0m\n",
      "\u001b[34mStarted processing for /tmp/main2 with 1 GPUs\u001b[0m\n",
      "\u001b[34m0%|          | 0/37 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1/37 [00:01<00:58,  1.63s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/37 [00:02<00:50,  1.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/37 [00:04<00:51,  1.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 4/37 [00:05<00:41,  1.26s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 5/37 [00:05<00:30,  1.06it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 6/37 [00:06<00:25,  1.21it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 7/37 [00:07<00:24,  1.21it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 8/37 [00:08<00:23,  1.23it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 9/37 [00:08<00:18,  1.51it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 10/37 [00:09<00:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 11/37 [00:10<00:23,  1.12it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 12/37 [00:10<00:18,  1.33it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 13/37 [00:11<00:20,  1.18it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 14/37 [00:12<00:16,  1.38it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 15/37 [00:12<00:15,  1.43it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 16/37 [00:13<00:14,  1.41it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 17/37 [00:14<00:14,  1.39it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 18/37 [00:14<00:11,  1.71it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 19/37 [00:15<00:10,  1.70it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 20/37 [00:15<00:08,  1.98it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 21/37 [00:16<00:09,  1.69it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 22/37 [00:17<00:09,  1.64it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 23/37 [00:17<00:07,  1.84it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 24/37 [00:18<00:07,  1.77it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 25/37 [00:19<00:09,  1.32it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 26/37 [00:19<00:07,  1.44it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 27/37 [00:20<00:06,  1.57it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 28/37 [00:20<00:04,  1.89it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 29/37 [00:21<00:04,  1.84it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 30/37 [00:22<00:04,  1.47it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 31/37 [00:22<00:04,  1.43it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 32/37 [00:23<00:03,  1.55it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 33/37 [00:24<00:02,  1.44it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 34/37 [00:24<00:02,  1.45it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 35/37 [00:25<00:01,  1.64it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 36/37 [00:26<00:00,  1.33it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 37/37 [00:27<00:00,  1.43it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 37/37 [00:27<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34mDumping audios...\u001b[0m\n",
      "\u001b[34m0%|          | 0/37 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 1/37 [00:00<00:04,  7.42it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/37 [00:00<00:04,  7.72it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/37 [00:00<00:04,  7.80it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 4/37 [00:00<00:04,  7.87it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 5/37 [00:00<00:04,  7.92it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 6/37 [00:00<00:03,  7.93it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 7/37 [00:00<00:03,  7.94it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 8/37 [00:01<00:03,  7.95it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 9/37 [00:01<00:03,  7.96it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 10/37 [00:01<00:03,  7.97it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 11/37 [00:01<00:03,  7.96it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 12/37 [00:01<00:03,  7.95it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 13/37 [00:01<00:03,  7.94it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 14/37 [00:01<00:02,  7.93it/s]\u001b[0m\n",
      "\u001b[34m41%|████      | 15/37 [00:01<00:02,  7.93it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 16/37 [00:02<00:02,  7.93it/s]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 17/37 [00:02<00:02,  7.93it/s]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 18/37 [00:02<00:02,  7.94it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 19/37 [00:02<00:02,  7.94it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 20/37 [00:02<00:02,  7.93it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 21/37 [00:02<00:02,  7.91it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 22/37 [00:02<00:01,  7.90it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 23/37 [00:02<00:01,  7.89it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 24/37 [00:03<00:01,  7.88it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 25/37 [00:03<00:01,  7.85it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 26/37 [00:03<00:01,  7.85it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 27/37 [00:03<00:01,  7.88it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 28/37 [00:03<00:01,  7.88it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 29/37 [00:03<00:01,  7.88it/s]\u001b[0m\n",
      "\u001b[34m81%|████████  | 30/37 [00:03<00:00,  7.86it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 31/37 [00:03<00:00,  7.87it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 32/37 [00:04<00:00,  7.88it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 33/37 [00:04<00:00,  7.87it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 34/37 [00:04<00:00,  7.90it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 35/37 [00:04<00:00,  7.93it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 36/37 [00:04<00:00,  7.89it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 37/37 [00:04<00:00,  7.90it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 37/37 [00:04<00:00,  7.90it/s]\u001b[0m\n",
      "\u001b[34mfinished preprocess\u001b[0m\n",
      "\u001b[34m--------prepare tain list--------\u001b[0m\n",
      "\u001b[34mfinished train list prepare\u001b[0m\n",
      "\u001b[34m--------train the expert discriminator------\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:47,674] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:47,727] [INFO] [runner.py:438:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 ./color_syncnet_dist_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_syncnet/ --deepspeed --deepspeed_config ds_config.json\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:96:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:96:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:96:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:123:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:49,008] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtrain_dataset: 66\u001b[0m\n",
      "\u001b[34mtest_dataset: 66\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,094] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,126] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,127] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,145] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,259] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,273] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,280] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,289] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34mtotal trainable params 19881059\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:53,292] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.1+06f2048, git-hash=06f2048, git-branch=HEAD\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:429: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,384] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,384] [INFO] [engine.py:1082:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,384] [INFO] [engine.py:1088:_configure_optimizer] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,393] [INFO] [engine.py:1104:_configure_optimizer] DeepSpeed Basic Optimizer = Adam\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,393] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,393] [INFO] [engine.py:794:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,393] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f8c03d7d3a0>\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,393] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,393] [INFO] [config.py:1069:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   curriculum_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   curriculum_params ............ False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,394] [INFO] [config.py:1073:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   fp16_mixed_quantize .......... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   gradient_accumulation_steps .. 2\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   gradient_clipping ............ 0.0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   initial_dynamic_scale ........ 4294967296\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   optimizer_name ............... adam\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   quantize_change_rate ......... 0.001\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   quantize_groups .............. 1\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,395] [INFO] [config.py:1073:print]   quantize_offset .............. 1000\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_period .............. 1000\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_rounding ............ 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_start_bits .......... 16\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_target_bits ......... 8\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_training_enabled .... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_type ................ 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   quantize_verbose ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   tensorboard_enabled .......... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   tensorboard_job_name ......... DeepSpeedJobName\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   tensorboard_output_path ...... \u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   train_batch_size ............. 64\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   train_micro_batch_size_per_gpu  4\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   use_quantizer_kernel ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": false, \n",
      "    \"contiguous_parameters\": false, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+08, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+08, \n",
      "    \"overlap_comm\": false, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false, \n",
      "    \"comm_small_bucket_size\": 5.000000e+08, \n",
      "    \"zero2d_local_shard\": false, \n",
      "    \"zero2d_shard_size\": -1, \n",
      "    \"zero2d_hierarchy_allgather\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   zero_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,396] [INFO] [config.py:1073:print]   zero_optimization_stage ...... 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:18:55,397] [INFO] [config.py:1075:print]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"auto_cast\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 2, \n",
      "    \"gradient_clipping\": 0.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 64, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.740891933441162 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.623562812805176 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.622722864151001 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.623462200164795 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.62312364578247 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.72371506690979 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.623561382293701 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 15.722883939743042 seconds\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.302: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.333 algo-1:8687 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.343: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.344: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.344: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.375 algo-1:8693 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.375 algo-1:8689 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.376 algo-1:8692 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.386: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.396: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.400: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.418 algo-1:8691 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.428 algo-1:8688 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.431: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.432 algo-1:8694 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.463 algo-1:8690 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.684 algo-1:8687 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.708 algo-1:8689 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.728 algo-1:8692 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.732 algo-1:8693 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.807 algo-1:8691 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.807 algo-1:8694 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.814 algo-1:8688 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:12.820 algo-1:8690 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]#0150it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9112515449523926: : 0it [00:04, ?it/s]#015Loss: 0.9112515449523926: : 1it [00:04,  4.42s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1707504987716675: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.1707504987716675: : 1it [00:04,  4.62s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.021824836730957: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.021824836730957: : 1it [00:04,  4.68s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0526652336120605: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0526652336120605: : 1it [00:04,  4.90s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8947451114654541: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8947451114654541: : 1it [00:05,  5.43s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.763810932636261: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.763810932636261: : 1it [00:05,  5.53s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9609366059303284: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9609366059303284: : 1it [00:05,  5.74s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1404647827148438: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.1404647827148438: : 1it [00:05,  5.84s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0015699565410614: : 1it [00:05,  4.90s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9561629891395569: : 1it [00:05,  4.62s/it]#015Loss: 0.9704481661319733: : 1it [00:05,  5.84s/it]#015Loss: 1.0015699565410614: : 2it [00:05,  2.64s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9704481661319733: : 2it [00:05,  2.46s/it]#015Loss: 0.9561629891395569: : 2it [00:05,  2.68s/it]#015Loss: 0.7018138468265533: : 1it [00:05,  5.53s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7018138468265533: : 2it [00:05,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1737244129180908: : 1it [00:06,  5.43s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1737244129180908: : 2it [00:06,  2.58s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6093766540288925: : 1it [00:05,  5.74s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6093766540288925: : 2it [00:05,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2138586640357971: : 1it [00:05,  4.68s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2138586640357971: : 2it [00:05,  2.66s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6105956435203552: : 1it [00:05,  4.42s/it]#015Loss: 0.6105956435203552: : 2it [00:05,  2.71s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7018138468265533: : 2it [00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0015699565410614: : 2it [00:06,  3.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6093766540288925: : 2it [00:06,  3.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9704481661319733: : 2it [00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1737244129180908: : 2it [00:06,  3.05s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6105956435203552: : 2it [00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9561629891395569: : 2it [00:06,  3.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2138586640357971: : 2it [00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.3447258472442627: : 0it [00:02, ?it/s]#015Loss: 1.3447258472442627: : 1it [00:02,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9893894195556641: : 0it [00:03, ?it/s]#015Loss: 0.9893894195556641: : 1it [00:03,  3.13s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0071152448654175: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0071152448654175: : 1it [00:03,  3.35s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.171121597290039: : 0it [00:03, ?it/s]#015Loss: 1.171121597290039: : 1it [00:03,  3.45s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7685644030570984: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.7685644030570984: : 1it [00:03,  3.55s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8441709280014038: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8441709280014038: : 1it [00:03,  3.96s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8773675560951233: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8773675560951233: : 1it [00:03,  3.92s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1903458833694458: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.1903458833694458: : 1it [00:04,  4.10s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8575625419616699: : 1it [00:04,  3.55s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8575625419616699: : 2it [00:04,  1.83s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9245651364326477: : 1it [00:04,  3.35s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9245651364326477: : 2it [00:04,  1.90s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7381217777729034: : 1it [00:04,  3.45s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7381217777729034: : 2it [00:04,  1.86s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8518946766853333: : 1it [00:04,  3.92s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8518946766853333: : 2it [00:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7999351918697357: : 1it [00:04,  3.96s/it]#015Loss: 0.7999351918697357: : 2it [00:04,  1.78s/it]#015Loss: 0.9835578203201294: : 1it [00:04,  3.13s/it]#015Loss: 1.1563672423362732: : 1it [00:04,  4.10s/it]#015Loss: 0.9835578203201294: : 2it [00:04,  1.93s/it]#015Loss: 1.1563672423362732: : 2it [00:04,  1.75s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.3090211153030396: : 1it [00:04,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.3090211153030396: : 2it [00:04,  2.00s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9245651364326477: : 2it [00:04,  2.17s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7381217777729034: : 2it [00:04,  2.16s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1563672423362732: : 2it [00:04,  2.17s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9835578203201294: : 2it [00:04,  2.18s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8575625419616699: : 2it [00:04,  2.16s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.3090211153030396: : 2it [00:04,  2.16s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7999351918697357: : 2it [00:04,  2.18s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8518946766853333: : 2it [00:04,  2.15s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.1128686666488647: : 0it [00:02, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.1128686666488647: : 1it [00:02,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0498487949371338: : 0it [00:03, ?it/s]#015Loss: 1.0498487949371338: : 1it [00:03,  3.66s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.154125690460205: : 0it [00:03, ?it/s]#015Loss: 1.154125690460205: : 1it [00:03,  3.91s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0801255702972412: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0801255702972412: : 1it [00:03,  3.97s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9046330451965332: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9046330451965332: : 1it [00:04,  4.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0731909275054932: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0731909275054932: : 1it [00:04,  4.11s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9986788034439087: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9986788034439087: : 1it [00:04,  4.43s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.158864974975586: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.158864974975586: : 1it [00:04,  4.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9711921215057373: : 1it [00:04,  4.11s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9711921215057373: : 2it [00:04,  2.13s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2133653163909912: : 1it [00:04,  3.66s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2133653163909912: : 2it [00:04,  2.28s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9812256693840027: : 1it [00:04,  4.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9812256693840027: : 2it [00:04,  2.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8791910409927368: : 1it [00:04,  4.02s/it]#015Loss: 0.9311721324920654: : 1it [00:04,  3.97s/it]#015Loss: 0.8601844310760498: : 1it [00:04,  4.43s/it]#015Loss: 0.8791910409927368: : 2it [00:04,  2.14s/it]#015Loss: 0.9311721324920654: : 2it [00:04,  2.13s/it]#015Loss: 0.8601844310760498: : 2it [00:04,  2.06s/it]#015Loss: 0.9453233182430267: : 1it [00:04,  2.60s/it]#015Loss: 0.9453233182430267: : 2it [00:04,  2.35s/it]#015Loss: 1.2932087182998657: : 1it [00:04,  3.91s/it]#015Loss: 1.2932087182998657: : 2it [00:04,  2.15s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2133653163909912: : 2it [00:05,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9453233182430267: : 2it [00:04,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9812256693840027: : 2it [00:04,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2932087182998657: : 2it [00:04,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8791910409927368: : 2it [00:04,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9311721324920654: : 2it [00:04,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9711921215057373: : 2it [00:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8601844310760498: : 2it [00:05,  2.50s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8603605628013611: : 0it [00:03, ?it/s]#015Loss: 0.8603605628013611: : 1it [00:03,  3.77s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8849525451660156: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8849525451660156: : 1it [00:04,  4.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.07326078414917: : 0it [00:04, ?it/s]#015Loss: 1.07326078414917: : 1it [00:04,  4.29s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.015764594078064: : 0it [00:04, ?it/s]#015Loss: 1.015764594078064: : 1it [00:04,  4.47s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8757892847061157: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8757892847061157: : 1it [00:04,  4.41s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0471470355987549: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0471470355987549: : 1it [00:04,  4.63s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.950829029083252: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.950829029083252: : 1it [00:04,  4.58s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8656818270683289: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8656818270683289: : 1it [00:05,  5.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8103926777839661: : 1it [00:05,  4.01s/it]#015Loss: 0.9582407772541046: : 1it [00:05,  4.29s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8103926777839661: : 2it [00:05,  2.26s/it]#015Loss: 0.9582407772541046: : 2it [00:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9713494181632996: : 1it [00:05,  4.63s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9713494181632996: : 2it [00:05,  2.24s/it]#015Loss: 0.8422461450099945: : 1it [00:05,  3.77s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8422461450099945: : 2it [00:05,  2.33s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8284279406070709: : 1it [00:05,  5.02s/it]#015Loss: 0.8472862541675568: : 1it [00:05,  4.58s/it]#015Loss: 0.8284279406070709: : 2it [00:05,  2.13s/it]#015Loss: 0.8472862541675568: : 2it [00:05,  2.18s/it]#015Loss: 0.8664760589599609: : 1it [00:05,  4.41s/it]#015Loss: 0.8664760589599609: : 2it [00:05,  2.20s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1953739523887634: : 1it [00:05,  4.47s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1953739523887634: : 2it [00:05,  2.25s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8103926777839661: : 2it [00:05,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9713494181632996: : 2it [00:05,  2.67s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8664760589599609: : 2it [00:05,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1953739523887634: : 2it [00:05,  2.66s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8284279406070709: : 2it [00:05,  2.64s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8422461450099945: : 2it [00:05,  2.63s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9582407772541046: : 2it [00:05,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8472862541675568: : 2it [00:05,  2.63s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0080807209014893: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0080807209014893: : 1it [00:03,  3.43s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0083603858947754: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0083603858947754: : 1it [00:03,  3.42s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8046269416809082: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8046269416809082: : 1it [00:03,  3.54s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1918678283691406: : 0it [00:03, ?it/s]#015Loss: 1.1918678283691406: : 1it [00:03,  3.77s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.023984432220459: : 0it [00:04, ?it/s]#015Loss: 1.023984432220459: : 1it [00:04,  4.05s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9761929512023926: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9761929512023926: : 1it [00:04,  4.21s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.167142629623413: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.167142629623413: : 1it [00:04,  4.28s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0051853656768799: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0051853656768799: : 1it [00:04,  4.53s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:19:39,971] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: /tmp/trained_syncnet/global_step5/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mSaved checkpoint:Saved checkpoint:Saved checkpoint:  /tmp/trained_syncnet//tmp/trained_syncnet/ /tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34mSaved checkpoint:Saved checkpoint:Saved checkpoint:\u001b[0m\n",
      "\u001b[34m/tmp/trained_syncnet/Saved checkpoint:\n",
      " /tmp/trained_syncnet/\n",
      "  /tmp/trained_syncnet//tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34mLoss: 0.9304236471652985: : 1it [00:05,  3.43s/it]#015Loss: 0.89446160197258: : 1it [00:05,  4.05s/it] #015Loss: 0.9304236471652985: : 2it [00:05,  2.38s/it]#015Loss: 0.89446160197258: : 2it [00:05,  2.28s/it]#015Loss: 1.2978640794754028: : 1it [00:05,  4.28s/it]#015Loss: 1.2978640794754028: : 2it [00:05,  2.22s/it]#015Loss: 0.8868426382541656: : 1it [00:05,  4.53s/it]#015Loss: 1.2808491587638855: : 1it [00:05,  3.77s/it]#015Loss: 1.1088830828666687: : 1it [00:05,  3.42s/it]#015Loss: 0.6571347117424011: : 1it [00:05,  4.21s/it]#015Loss: 0.8868426382541656: : 2it [00:05,  2.18s/it]#015Loss: 1.2808491587638855: : 2it [00:05,  2.38s/it]#015Loss: 1.1088830828666687: : 2it [00:05,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6571347117424011: : 2it [00:05,  2.26s/it]#015Loss: 0.7924563884735107: : 1it [00:05,  3.54s/it]\u001b[0m\n",
      "\u001b[34mSaved checkpoint: /tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34mLoss: 0.7924563884735107: : 2it [00:05,  2.38s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.89446160197258: : 2it [00:05,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2808491587638855: : 2it [00:05,  2.66s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2978640794754028: : 2it [00:05,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7924563884735107: : 2it [00:05,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8868426382541656: : 2it [00:05,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1088830828666687: : 2it [00:05,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9304236471652985: : 2it [00:05,  2.61s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6571347117424011: : 2it [00:05,  2.63s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0558151006698608: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0558151006698608: : 1it [00:03,  3.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0567494630813599: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0567494630813599: : 1it [00:03,  3.13s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8709855079650879: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8709855079650879: : 1it [00:03,  3.23s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9943766593933105: : 0it [00:03, ?it/s]#015Loss: 0.9943766593933105: : 1it [00:03,  3.37s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9662429690361023: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9662429690361023: : 1it [00:03,  3.65s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0237610340118408: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0237610340118408: : 1it [00:04,  4.33s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9080438613891602: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9080438613891602: : 1it [00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8949034810066223: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8949034810066223: : 1it [00:04,  4.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.151366114616394: : 1it [00:04,  3.37s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.151366114616394: : 2it [00:04,  2.25s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9185489416122437: : 1it [00:04,  3.13s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9185489416122437: : 2it [00:04,  2.27s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6123798489570618: : 1it [00:04,  4.48s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6123798489570618: : 2it [00:04,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7074176669120789: : 1it [00:04,  4.33s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7074176669120789: : 2it [00:04,  2.13s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.845348060131073: : 1it [00:04,  3.23s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.668176144361496: : 1it [00:04,  3.65s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.668176144361496: : 2it [00:04,  2.18s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8540052473545074: : 1it [00:04,  4.72s/it]#015Loss: 0.8540052473545074: : 2it [00:04,  2.01s/it]#015Loss: 0.845348060131073: : 2it [00:04,  2.27s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9175878763198853: : 1it [00:04,  3.02s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9175878763198853: : 2it [00:04,  2.30s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7074176669120789: : 2it [00:05,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9175878763198853: : 2it [00:04,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.668176144361496: : 2it [00:04,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.845348060131073: : 2it [00:04,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.151366114616394: : 2it [00:04,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9185489416122437: : 2it [00:04,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8540052473545074: : 2it [00:04,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6123798489570618: : 2it [00:05,  2.52s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.2815438508987427: : 0it [00:03, ?it/s]#015Loss: 1.2815438508987427: : 1it [00:03,  3.24s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8915744423866272: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8915744423866272: : 1it [00:03,  3.74s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0066032409667969: : 0it [00:04, ?it/s]#015Loss: 1.0066032409667969: : 1it [00:04,  4.46s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.013762354850769: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.013762354850769: : 1it [00:04,  4.62s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0110676288604736: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0110676288604736: : 1it [00:04,  4.99s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9422459602355957: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9422459602355957: : 1it [00:04,  4.99s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9155349731445312: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9155349731445312: : 1it [00:05,  5.14s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.944021463394165: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.944021463394165: : 1it [00:05,  5.36s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8699261844158173: : 1it [00:05,  5.36s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8699261844158173: : 2it [00:05,  2.27s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1970323324203491: : 1it [00:05,  4.99s/it]#015Loss: 0.67989581823349: : 1it [00:05,  4.62s/it] #015Loss: 1.1970323324203491: : 2it [00:05,  2.27s/it]#015Loss: 0.67989581823349: : 2it [00:05,  2.33s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0310556888580322: : 1it [00:05,  3.24s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0310556888580322: : 2it [00:05,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8483558595180511: : 1it [00:05,  5.14s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8483558595180511: : 2it [00:05,  2.24s/it]#015Loss: 1.1168508529663086: : 1it [00:05,  4.99s/it]#015Loss: 1.1168508529663086: : 2it [00:05,  2.24s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0341953337192535: : 1it [00:05,  3.74s/it]#015Loss: 0.7937496900558472: : 1it [00:05,  4.46s/it]#015Loss: 1.0341953337192535: : 2it [00:05,  2.48s/it]#015Loss: 0.7937496900558472: : 2it [00:05,  2.40s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8699261844158173: : 2it [00:05,  2.79s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7937496900558472: : 2it [00:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1970323324203491: : 2it [00:05,  2.75s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1168508529663086: : 2it [00:05,  2.73s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.67989581823349: : 2it [00:05,  2.76s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0310556888580322: : 2it [00:05,  2.75s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8483558595180511: : 2it [00:05,  2.76s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0341953337192535: : 2it [00:05,  2.76s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0909229516983032: : 0it [00:03, ?it/s]#015Loss: 1.0909229516983032: : 1it [00:03,  3.25s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8953039646148682: : 0it [00:03, ?it/s]#015Loss: 0.8953039646148682: : 1it [00:03,  3.54s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8616191148757935: : 0it [00:03, ?it/s]#015Loss: 0.8616191148757935: : 1it [00:03,  3.85s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9049542546272278: : 0it [00:04, ?it/s]#015Loss: 0.9049542546272278: : 1it [00:04,  4.10s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0548940896987915: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0548940896987915: : 1it [00:04,  4.09s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.999595582485199: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.999595582485199: : 1it [00:04,  4.20s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.090165376663208: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.090165376663208: : 1it [00:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0111212730407715: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0111212730407715: : 1it [00:04,  4.40s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8597392141819: : 1it [00:04,  4.10s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8597392141819: : 2it [00:04,  1.82s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6868579685688019: : 1it [00:04,  4.18s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6868579685688019: : 2it [00:04,  1.81s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9265255331993103: : 1it [00:04,  4.40s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9265255331993103: : 2it [00:04,  1.87s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0785657167434692: : 1it [00:04,  3.85s/it]#015Loss: 1.0785657167434692: : 2it [00:04,  1.88s/it]#015Loss: 0.8614263236522675: : 1it [00:04,  3.54s/it]#015Loss: 0.8614263236522675: : 2it [00:04,  1.95s/it]#015Loss: 1.2558781802654266: : 1it [00:04,  4.20s/it]#015Loss: 1.2558781802654266: : 2it [00:04,  1.84s/it]#015Loss: 0.9699475169181824: : 1it [00:04,  4.09s/it]#015Loss: 0.9699475169181824: : 2it [00:04,  1.82s/it]#015Loss: 0.9298108220100403: : 1it [00:04,  3.25s/it]#015Loss: 0.9298108220100403: : 2it [00:04,  1.98s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.2558781802654266: : 2it [00:04,  2.26s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9699475169181824: : 2it [00:04,  2.23s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8597392141819: : 2it [00:04,  2.24s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6868579685688019: : 2it [00:04,  2.25s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9265255331993103: : 2it [00:04,  2.34s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0785657167434692: : 2it [00:04,  2.26s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8614263236522675: : 2it [00:04,  2.27s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9298108220100403: : 2it [00:04,  2.25s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9336549043655396: : 0it [00:03, ?it/s]#015Loss: 0.9336549043655396: : 1it [00:03,  3.80s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9218623638153076: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9218623638153076: : 1it [00:03,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8755104541778564: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.8755104541778564: : 1it [00:03,  3.84s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9319801330566406: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9319801330566406: : 1it [00:03,  3.92s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1023967266082764: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.1023967266082764: : 1it [00:04,  4.82s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0159783363342285: : 0it [00:05, ?it/s]#015Loss: 1.0159783363342285: : 1it [00:05,  5.07s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9399495124816895: : 0it [00:05, ?it/s]#015Loss: 0.9399495124816895: : 1it [00:05,  5.36s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0065821409225464: : 0it [00:05, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0065821409225464: : 1it [00:05,  5.73s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8904174566268921: : 1it [00:05,  3.92s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8904174566268921: : 2it [00:05,  2.73s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9938187599182129: : 1it [00:05,  5.36s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9938187599182129: : 2it [00:05,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.209865927696228: : 1it [00:05,  3.90s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.209865927696228: : 2it [00:05,  2.76s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6084683984518051: : 1it [00:05,  3.80s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6084683984518051: : 2it [00:05,  2.78s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6397120654582977: : 1it [00:05,  5.73s/it]#015Loss: 0.8822559714317322: : 1it [00:05,  5.07s/it]#015Loss: 0.8822559714317322: : 2it [00:05,  2.56s/it]#015Loss: 0.6397120654582977: : 2it [00:05,  2.42s/it]#015Loss: 0.8166917562484741: : 1it [00:05,  3.84s/it]#015Loss: 0.8166917562484741: : 2it [00:05,  2.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.164961814880371: : 1it [00:05,  4.82s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.164961814880371: : 2it [00:05,  2.65s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6397120654582977: : 2it [00:05,  2.99s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9938187599182129: : 2it [00:05,  2.99s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8822559714317322: : 2it [00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.164961814880371: : 2it [00:06,  3.04s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8904174566268921: : 2it [00:05,  2.99s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8166917562484741: : 2it [00:05,  2.97s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6084683984518051: : 2it [00:06,  3.01s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.209865927696228: : 2it [00:06,  3.02s/it]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9246211647987366: : 0it [00:03, ?it/s]#015Loss: 0.9246211647987366: : 1it [00:03,  3.36s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9744465351104736: : 0it [00:03, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9744465351104736: : 1it [00:03,  3.55s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9785475730895996: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9785475730895996: : 1it [00:04,  4.06s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.014151692390442: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.014151692390442: : 1it [00:04,  4.10s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0452805757522583: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 1.0452805757522583: : 1it [00:04,  4.16s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.7856183052062988: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.7856183052062988: : 1it [00:04,  4.21s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9696236848831177: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9696236848831177: : 1it [00:04,  4.46s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9946528673171997: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mLoss: 0.9946528673171997: : 1it [00:04,  4.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:08,385] [INFO] [logging.py:69:log_dist] [Rank 0] Saving model checkpoint: /tmp/trained_syncnet/global_step10/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mSaved checkpoint:Saved checkpoint:Saved checkpoint:Saved checkpoint:Saved checkpoint:Saved checkpoint:Saved checkpoint:    /tmp/trained_syncnet/ /tmp/trained_syncnet/ /tmp/trained_syncnet//tmp/trained_syncnet//tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34m/tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34m/tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34mLoss: 1.0452773571014404: : 1it [00:05,  4.46s/it]#015Loss: 0.6669720709323883: : 1it [00:05,  4.16s/it]#015Loss: 1.139516294002533: : 1it [00:05,  4.89s/it] #015Loss: 1.1000625789165497: : 1it [00:05,  3.36s/it]#015Loss: 0.8771932721138: : 1it [00:05,  4.06s/it]   #015Loss: 1.0452773571014404: : 2it [00:05,  2.32s/it]#015Loss: 0.900259256362915: : 1it [00:05,  3.55s/it] #015Loss: 0.9375469386577606: : 1it [00:05,  4.10s/it]#015Loss: 1.139516294002533: : 2it [00:05,  2.23s/it]#015Loss: 0.6669720709323883: : 2it [00:05,  2.38s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8771932721138: : 2it [00:05,  2.40s/it]#015Loss: 1.1000625789165497: : 2it [00:05,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9375469386577606: : 2it [00:05,  2.38s/it]#015Loss: 0.900259256362915: : 2it [00:05,  2.50s/it]\u001b[0m\n",
      "\u001b[34mSaved checkpoint: /tmp/trained_syncnet/\u001b[0m\n",
      "\u001b[34mLoss: 0.8012972474098206: : 1it [00:05,  4.21s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8012972474098206: : 2it [00:05,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8012972474098206: : 2it [00:05,  2.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.8771932721138: : 2it [00:05,  2.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.900259256362915: : 2it [00:05,  2.74s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.1000625789165497: : 2it [00:05,  2.75s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.139516294002533: : 2it [00:05,  2.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.9375469386577606: : 2it [00:05,  2.72s/it]\u001b[0m\n",
      "\u001b[34mLoss: 0.6669720709323883: : 2it [00:05,  2.74s/it]\u001b[0m\n",
      "\u001b[34mLoss: 1.0452773571014404: : 2it [00:05,  2.74s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:10,130] [INFO] [launch.py:210:main] Process 8687 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,132] [INFO] [launch.py:210:main] Process 8694 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,132] [INFO] [launch.py:210:main] Process 8693 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,132] [INFO] [launch.py:210:main] Process 8691 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,133] [INFO] [launch.py:210:main] Process 8692 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,133] [INFO] [launch.py:210:main] Process 8688 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,133] [INFO] [launch.py:210:main] Process 8690 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:11,134] [INFO] [launch.py:210:main] Process 8689 exits successfully.\u001b[0m\n",
      "\u001b[34m时间间隔为 86 秒\u001b[0m\n",
      "\u001b[34mfinished train syncnet\u001b[0m\n",
      "\u001b[34mtrained_syncnet_model_file: /tmp/trained_syncnet/global_step5/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m--------train wav2lip-----------------------\u001b[0m\n",
      "\u001b[34muse_cuda: True\u001b[0m\n",
      "\u001b[34mtotal trainable params 48520815\u001b[0m\n",
      "\u001b[34mtotal DISC trainable params 18210561\u001b[0m\n",
      "\u001b[34mLoad checkpoint from: /tmp/trained_syncnet/global_step5/mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34mStarting Epoch: 0\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:17.016: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:17.046 algo-1:63468 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-21 08:20:17.296 algo-1:63468 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mSaved checkpoint: /tmp/trained_wav2lip_288x288/checkpoint_step000000001.pth\u001b[0m\n",
      "\u001b[34mSaved checkpoint: /tmp/trained_wav2lip_288x288/disc_checkpoint_step000000001.pth\u001b[0m\n",
      "\u001b[34mL1: 0.20726631581783295, Sync: 0.0, Percep: 0.6835467219352722 | Fake: 0.7028408050537109, Real: 0.6835466623306274: : 0it [00:04, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.20726631581783295, Sync: 0.0, Percep: 0.6835467219352722 | Fake: 0.7028408050537109, Real: 0.6835466623306274: : 1it [00:04,  4.73s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.2165011763572693, Sync: 0.0, Percep: 0.6855230927467346 | Fake: 0.7008338868618011, Real: 0.6855230033397675: : 1it [00:05,  4.73s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.2165011763572693, Sync: 0.0, Percep: 0.6855230927467346 | Fake: 0.7008338868618011, Real: 0.6855230033397675: : 2it [00:05,  2.27s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.2056067238251368, Sync: 0.0, Percep: 0.687336802482605 | Fake: 0.699000895023346, Real: 0.6873367428779602: : 2it [00:05,  2.27s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.2056067238251368, Sync: 0.0, Percep: 0.687336802482605 | Fake: 0.699000895023346, Real: 0.6873367428779602: : 3it [00:05,  1.49s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.2057023048400879, Sync: 0.0, Percep: 0.6889322102069855 | Fake: 0.6973947584629059, Real: 0.6889321208000183: : 3it [00:06,  1.49s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.2057023048400879, Sync: 0.0, Percep: 0.6889322102069855 | Fake: 0.6973947584629059, Real: 0.6889321208000183: : 4it [00:06,  1.12s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.20513657331466675, Sync: 0.0, Percep: 0.6901126980781556 | Fake: 0.6962083220481873, Real: 0.6901125073432922: : 4it [00:06,  1.12s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.20513657331466675, Sync: 0.0, Percep: 0.6901126980781556 | Fake: 0.6962083220481873, Real: 0.6901125073432922: : 5it [00:06,  1.09it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.199855109055837, Sync: 0.0, Percep: 0.69080917040507 | Fake: 0.6955076257387797, Real: 0.6908088028430939: : 5it [00:07,  1.09it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.199855109055837, Sync: 0.0, Percep: 0.69080917040507 | Fake: 0.6955076257387797, Real: 0.6908088028430939: : 6it [00:07,  1.26it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.19390486180782318, Sync: 0.0, Percep: 0.6911822983196804 | Fake: 0.6951313018798828, Real: 0.6911805697849819: : 6it [00:08,  1.26it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.19390486180782318, Sync: 0.0, Percep: 0.6911822983196804 | Fake: 0.6951313018798828, Real: 0.6911805697849819: : 7it [00:08,  1.40it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.19840438105165958, Sync: 0.0, Percep: 0.691448263823986 | Fake: 0.6948629319667816, Real: 0.6914333403110504: : 7it [00:08,  1.40it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.19840438105165958, Sync: 0.0, Percep: 0.691448263823986 | Fake: 0.6948629319667816, Real: 0.6914333403110504: : 8it [00:08,  1.51it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1941391477982203, Sync: 0.0, Percep: 0.6918700072500441 | Fake: 0.6944398151503669, Real: 0.6917464203304715: : 8it [00:09,  1.51it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1941391477982203, Sync: 0.0, Percep: 0.6918700072500441 | Fake: 0.6944398151503669, Real: 0.6917464203304715: : 9it [00:09,  1.58it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.18874188363552094, Sync: 0.0, Percep: 0.6930038630962372 | Fake: 0.6933144390583038, Real: 0.692755937576294: : 9it [00:09,  1.58it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.18874188363552094, Sync: 0.0, Percep: 0.6930038630962372 | Fake: 0.6933144390583038, Real: 0.692755937576294: : 10it [00:09,  1.64it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.18458538163792004, Sync: 0.0, Percep: 0.6961106007749384 | Fake: 0.6903075521642511, Real: 0.6929490024393256: : 10it [00:10,  1.64it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.18458538163792004, Sync: 0.0, Percep: 0.6961106007749384 | Fake: 0.6903075521642511, Real: 0.6929490024393256: : 11it [00:10,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.18308082719643912, Sync: 0.0, Percep: 0.7261500010887781 | Fake: 0.6684515252709389, Real: 0.6949636787176132: : 11it [00:10,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.18308082719643912, Sync: 0.0, Percep: 0.7261500010887781 | Fake: 0.6684515252709389, Real: 0.6949636787176132: : 12it [00:10,  1.72it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1806173141186054, Sync: 0.0, Percep: 0.9905448005749629 | Fake: 0.6183110799353856, Real: 0.6937174430260291: : 12it [00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1806173141186054, Sync: 0.0, Percep: 0.9905448005749629 | Fake: 0.6183110799353856, Real: 0.6937174430260291: : 13it [00:11,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17862281096833094, Sync: 0.0, Percep: 3.4919403323105405 | Fake: 0.5741460027971438, Real: 0.7204977869987488: : 13it [00:11,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17862281096833094, Sync: 0.0, Percep: 3.4919403323105405 | Fake: 0.5741460027971438, Real: 0.7204977869987488: : 14it [00:11,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.177719850341479, Sync: 0.0, Percep: 3.2908689200878145 | Fake: 0.6007854384680589, Real: 0.7006483455499013: : 14it [00:12,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.177719850341479, Sync: 0.0, Percep: 3.2908689200878145 | Fake: 0.6007854384680589, Real: 0.7006483455499013: : 15it [00:12,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17565874103456736, Sync: 0.0, Percep: 3.121325084939599 | Fake: 0.6147015991155058, Real: 0.6920363549143076: : 15it [00:13,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17565874103456736, Sync: 0.0, Percep: 3.121325084939599 | Fake: 0.6147015991155058, Real: 0.6920363549143076: : 16it [00:13,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17490461556350484, Sync: 0.0, Percep: 2.9765124443699333 | Fake: 0.6213635699275661, Real: 0.6901372478288763: : 16it [00:13,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17490461556350484, Sync: 0.0, Percep: 2.9765124443699333 | Fake: 0.6213635699275661, Real: 0.6901372478288763: : 17it [00:13,  2.00it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.17490461556350484, Sync: 0.0, Percep: 2.9765124443699333 | Fake: 0.6213635699275661, Real: 0.6901372478288763: : 17it [00:13,  1.26it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 1\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.21359069645404816, Sync: 0.0, Percep: 0.629948079586029 | Fake: 0.7606254816055298, Real: 0.6281939744949341: : 0it [00:02, ?it/s]#015L1: 0.21359069645404816, Sync: 0.0, Percep: 0.629948079586029 | Fake: 0.7606254816055298, Real: 0.6281939744949341: : 1it [00:02,  2.19s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.17152869701385498, Sync: 0.0, Percep: 0.6386656165122986 | Fake: 0.7508682310581207, Real: 0.6359280645847321: : 1it [00:02,  2.19s/it]#015L1: 0.17152869701385498, Sync: 0.0, Percep: 0.6386656165122986 | Fake: 0.7508682310581207, Real: 0.6359280645847321: : 2it [00:02,  1.22s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1663645605246226, Sync: 0.0, Percep: 0.6357341408729553 | Fake: 0.7541653513908386, Real: 0.6324097315470377: : 2it [00:03,  1.22s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1663645605246226, Sync: 0.0, Percep: 0.6357341408729553 | Fake: 0.7541653513908386, Real: 0.6324097315470377: : 3it [00:03,  1.09it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.16256943345069885, Sync: 0.0, Percep: 0.6342775374650955 | Fake: 0.7558073997497559, Real: 0.6305724829435349: : 3it [00:03,  1.09it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.16256943345069885, Sync: 0.0, Percep: 0.6342775374650955 | Fake: 0.7558073997497559, Real: 0.6305724829435349: : 4it [00:03,  1.29it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.16051414906978606, Sync: 0.0, Percep: 0.6356676578521728 | Fake: 0.7542349100112915, Real: 0.6314329981803894: : 4it [00:04,  1.29it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.16051414906978606, Sync: 0.0, Percep: 0.6356676578521728 | Fake: 0.7542349100112915, Real: 0.6314329981803894: : 5it [00:04,  1.43it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.15426083529988924, Sync: 0.0, Percep: 0.6336304644743601 | Fake: 0.7565602560838064, Real: 0.6273942093054453: : 5it [00:04,  1.43it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.15426083529988924, Sync: 0.0, Percep: 0.6336304644743601 | Fake: 0.7565602560838064, Real: 0.6273942093054453: : 6it [00:04,  1.54it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.15102431284529821, Sync: 0.0, Percep: 0.6216979026794434 | Fake: 0.7714385390281677, Real: 0.6103769796235221: : 6it [00:05,  1.54it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.15102431284529821, Sync: 0.0, Percep: 0.6216979026794434 | Fake: 0.7714385390281677, Real: 0.6103769796235221: : 7it [00:05,  1.61it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.15154857281595469, Sync: 0.0, Percep: 0.6147099658846855 | Fake: 0.7799350023269653, Real: 0.5998586565256119: : 7it [00:06,  1.61it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.15154857281595469, Sync: 0.0, Percep: 0.6147099658846855 | Fake: 0.7799350023269653, Real: 0.5998586565256119: : 8it [00:06,  1.67it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14962288157807457, Sync: 0.0, Percep: 0.6097432110044692 | Fake: 0.7860099143452115, Real: 0.5928379760848151: : 8it [00:06,  1.67it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14962288157807457, Sync: 0.0, Percep: 0.6097432110044692 | Fake: 0.7860099143452115, Real: 0.5928379760848151: : 9it [00:06,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1471069149672985, Sync: 0.0, Percep: 0.6082257568836212 | Fake: 0.7877268195152283, Real: 0.5887525975704193: : 9it [00:07,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1471069149672985, Sync: 0.0, Percep: 0.6082257568836212 | Fake: 0.7877268195152283, Real: 0.5887525975704193: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14737075093117627, Sync: 0.0, Percep: 0.5979275161569769 | Fake: 0.8018323562361978, Real: 0.5739662457596172: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14737075093117627, Sync: 0.0, Percep: 0.5979275161569769 | Fake: 0.8018323562361978, Real: 0.5739662457596172: : 11it [00:07,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14545989160736403, Sync: 0.0, Percep: 0.600821832815806 | Fake: 0.7981439828872681, Real: 0.5754787301023802: : 11it [00:08,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14545989160736403, Sync: 0.0, Percep: 0.600821832815806 | Fake: 0.7981439828872681, Real: 0.5754787301023802: : 12it [00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14570771501614496, Sync: 0.0, Percep: 0.6003487935433021 | Fake: 0.7984973329764146, Real: 0.5710104680978335: : 12it [00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14570771501614496, Sync: 0.0, Percep: 0.6003487935433021 | Fake: 0.7984973329764146, Real: 0.5710104680978335: : 13it [00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1434810534119606, Sync: 0.0, Percep: 0.5880828308207648 | Fake: 0.8171634461198535, Real: 0.5475192038076264: : 13it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1434810534119606, Sync: 0.0, Percep: 0.5880828308207648 | Fake: 0.8171634461198535, Real: 0.5475192038076264: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1428508182366689, Sync: 0.0, Percep: 0.5921471138795217 | Fake: 0.8119765003522237, Real: 0.5478352000315984: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1428508182366689, Sync: 0.0, Percep: 0.5921471138795217 | Fake: 0.8119765003522237, Real: 0.5478352000315984: : 15it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14025655901059508, Sync: 0.0, Percep: 0.595554931089282 | Fake: 0.8076002113521099, Real: 0.5482582235708833: : 15it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14025655901059508, Sync: 0.0, Percep: 0.595554931089282 | Fake: 0.8076002113521099, Real: 0.5482582235708833: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14116176743717754, Sync: 0.0, Percep: 0.5889629861887764 | Fake: 0.8168742586584652, Real: 0.5196048275512808: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14116176743717754, Sync: 0.0, Percep: 0.5889629861887764 | Fake: 0.8168742586584652, Real: 0.5196048275512808: : 17it [00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14116176743717754, Sync: 0.0, Percep: 0.5889629861887764 | Fake: 0.8168742586584652, Real: 0.5196048275512808: : 17it [00:10,  1.55it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 2\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12494916468858719, Sync: 0.0, Percep: 0.6771396994590759 | Fake: 0.709415078163147, Real: 0.5451752543449402: : 0it [00:01, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12494916468858719, Sync: 0.0, Percep: 0.6771396994590759 | Fake: 0.709415078163147, Real: 0.5451752543449402: : 1it [00:01,  1.95s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11766970530152321, Sync: 0.0, Percep: 0.6781503856182098 | Fake: 0.7083736956119537, Real: 0.5331476330757141: : 1it [00:02,  1.95s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11766970530152321, Sync: 0.0, Percep: 0.6781503856182098 | Fake: 0.7083736956119537, Real: 0.5331476330757141: : 2it [00:02,  1.13s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11470960328976314, Sync: 0.0, Percep: 0.6877749760945638 | Fake: 0.6990321079889933, Real: 0.37974966565767926: : 2it [00:03,  1.13s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11470960328976314, Sync: 0.0, Percep: 0.6877749760945638 | Fake: 0.6990321079889933, Real: 0.37974966565767926: : 3it [00:03,  1.14it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11261818744242191, Sync: 0.0, Percep: 0.5692247152328491 | Fake: 1.8907690942287445, Real: 0.28481224924325943: : 3it [00:03,  1.14it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11261818744242191, Sync: 0.0, Percep: 0.5692247152328491 | Fake: 1.8907690942287445, Real: 0.28481224924325943: : 4it [00:03,  1.33it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12723489552736283, Sync: 0.0, Percep: 1.324371910095215 | Fake: 1.5157428797334433, Real: 0.4280713379383087: : 4it [00:04,  1.33it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12723489552736283, Sync: 0.0, Percep: 1.324371910095215 | Fake: 1.5157428797334433, Real: 0.4280713379383087: : 5it [00:04,  1.46it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.130575742572546, Sync: 0.0, Percep: 1.241296609242757 | Fake: 1.3591161913548906, Real: 0.4810231576363246: : 5it [00:04,  1.46it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.130575742572546, Sync: 0.0, Percep: 1.241296609242757 | Fake: 1.3591161913548906, Real: 0.4810231576363246: : 6it [00:04,  1.57it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1311487928032875, Sync: 0.0, Percep: 1.1738431368555342 | Fake: 1.2538914209497827, Real: 0.5184926773820605: : 6it [00:05,  1.57it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1311487928032875, Sync: 0.0, Percep: 1.1738431368555342 | Fake: 1.2538914209497827, Real: 0.5184926773820605: : 7it [00:05,  1.63it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1297122212126851, Sync: 0.0, Percep: 1.1237222403287888 | Fake: 1.1745693495031446, Real: 0.5481812991201878: : 7it [00:05,  1.63it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1297122212126851, Sync: 0.0, Percep: 1.1237222403287888 | Fake: 1.1745693495031446, Real: 0.5481812991201878: : 8it [00:05,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13000631249613231, Sync: 0.0, Percep: 1.0866392652193706 | Fake: 1.111270622867677, Real: 0.5719065368175507: : 8it [00:06,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13000631249613231, Sync: 0.0, Percep: 1.0866392652193706 | Fake: 1.111270622867677, Real: 0.5719065368175507: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12813679799437522, Sync: 0.0, Percep: 1.0607447266578673 | Fake: 1.0576056925579906, Real: 0.5937570005655288: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12813679799437522, Sync: 0.0, Percep: 1.0607447266578673 | Fake: 1.0576056925579906, Real: 0.5937570005655288: : 10it [00:06,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13253154741092163, Sync: 0.0, Percep: 1.041666643186049 | Fake: 1.0121012734757229, Real: 0.608551645820791: : 10it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13253154741092163, Sync: 0.0, Percep: 1.041666643186049 | Fake: 1.0121012734757229, Real: 0.608551645820791: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1352294118454059, Sync: 0.0, Percep: 1.0470746109882991 | Fake: 0.9612234556116164, Real: 0.6251962011059126: : 11it [00:08,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1352294118454059, Sync: 0.0, Percep: 1.0470746109882991 | Fake: 0.9612234556116164, Real: 0.6251962011059126: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13604373026352662, Sync: 0.0, Percep: 1.1639202328828664 | Fake: 0.8935409261343571, Real: 0.682329372717784: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13604373026352662, Sync: 0.0, Percep: 1.1639202328828664 | Fake: 0.8935409261343571, Real: 0.682329372717784: : 13it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1360806402351175, Sync: 0.0, Percep: 1.1302978949887412 | Fake: 0.8792228809158716, Real: 0.6826049664190837: : 13it [00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1360806402351175, Sync: 0.0, Percep: 1.1302978949887412 | Fake: 0.8792228809158716, Real: 0.6826049664190837: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13521344711383185, Sync: 0.0, Percep: 1.1003947377204895 | Fake: 0.8675863686949015, Real: 0.6825589557488759: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13521344711383185, Sync: 0.0, Percep: 1.1003947377204895 | Fake: 0.8675863686949015, Real: 0.6825589557488759: : 15it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1345366882160306, Sync: 0.0, Percep: 1.0742114074528217 | Fake: 0.8574229165678844, Real: 0.6825104150921106: : 15it [00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1345366882160306, Sync: 0.0, Percep: 1.0742114074528217 | Fake: 0.8574229165678844, Real: 0.6825104150921106: : 16it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13582072801449718, Sync: 0.0, Percep: 1.0511073960977442 | Fake: 0.848456258611644, Real: 0.6824444094124962: : 16it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13582072801449718, Sync: 0.0, Percep: 1.0511073960977442 | Fake: 0.848456258611644, Real: 0.6824444094124962: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13582072801449718, Sync: 0.0, Percep: 1.0511073960977442 | Fake: 0.848456258611644, Real: 0.6824444094124962: : 17it [00:10,  1.58it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 3\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09923315793275833, Sync: 0.0, Percep: 0.6815192699432373 | Fake: 0.7049119472503662, Real: 0.6814661622047424: : 0it [00:02, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09923315793275833, Sync: 0.0, Percep: 0.6815192699432373 | Fake: 0.7049119472503662, Real: 0.6814661622047424: : 1it [00:02,  2.05s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11885622516274452, Sync: 0.0, Percep: 0.6815772950649261 | Fake: 0.7048525512218475, Real: 0.6814886629581451: : 1it [00:02,  2.05s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11885622516274452, Sync: 0.0, Percep: 0.6815772950649261 | Fake: 0.7048525512218475, Real: 0.6814886629581451: : 2it [00:02,  1.17s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1199713001648585, Sync: 0.0, Percep: 0.6816349625587463 | Fake: 0.7047934929529825, Real: 0.6815277536710104: : 2it [00:03,  1.17s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1199713001648585, Sync: 0.0, Percep: 0.6816349625587463 | Fake: 0.7047934929529825, Real: 0.6815277536710104: : 3it [00:03,  1.11it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11565046012401581, Sync: 0.0, Percep: 0.6816987842321396 | Fake: 0.7047281861305237, Real: 0.681574359536171: : 3it [00:03,  1.11it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11565046012401581, Sync: 0.0, Percep: 0.6816987842321396 | Fake: 0.7047281861305237, Real: 0.681574359536171: : 4it [00:03,  1.31it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11632289886474609, Sync: 0.0, Percep: 0.6817655682563781 | Fake: 0.7046598792076111, Real: 0.6816393375396729: : 4it [00:04,  1.31it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11632289886474609, Sync: 0.0, Percep: 0.6817655682563781 | Fake: 0.7046598792076111, Real: 0.6816393375396729: : 5it [00:04,  1.45it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11483100180824597, Sync: 0.0, Percep: 0.681830366452535 | Fake: 0.7045935988426208, Real: 0.6816898981730143: : 5it [00:04,  1.45it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11483100180824597, Sync: 0.0, Percep: 0.681830366452535 | Fake: 0.7045935988426208, Real: 0.6816898981730143: : 6it [00:04,  1.55it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11487814038991928, Sync: 0.0, Percep: 0.6818973081452506 | Fake: 0.7045251386506217, Real: 0.6817334634917123: : 6it [00:05,  1.55it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11487814038991928, Sync: 0.0, Percep: 0.6818973081452506 | Fake: 0.7045251386506217, Real: 0.6817334634917123: : 7it [00:05,  1.61it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11354557983577251, Sync: 0.0, Percep: 0.6819639652967453 | Fake: 0.7044569849967957, Real: 0.6817982271313667: : 7it [00:05,  1.61it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11354557983577251, Sync: 0.0, Percep: 0.6819639652967453 | Fake: 0.7044569849967957, Real: 0.6817982271313667: : 8it [00:05,  1.67it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11503125230471294, Sync: 0.0, Percep: 0.6820308433638679 | Fake: 0.7043886184692383, Real: 0.6818168295754327: : 8it [00:06,  1.67it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11503125230471294, Sync: 0.0, Percep: 0.6820308433638679 | Fake: 0.7043886184692383, Real: 0.6818168295754327: : 9it [00:06,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1141520582139492, Sync: 0.0, Percep: 0.6820987403392792 | Fake: 0.7043192148208618, Real: 0.6819050312042236: : 9it [00:07,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1141520582139492, Sync: 0.0, Percep: 0.6820987403392792 | Fake: 0.7043192148208618, Real: 0.6819050312042236: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11369465020569888, Sync: 0.0, Percep: 0.6821655793623491 | Fake: 0.7042509154839949, Real: 0.6819400137121027: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11369465020569888, Sync: 0.0, Percep: 0.6821655793623491 | Fake: 0.7042509154839949, Real: 0.6819400137121027: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11494726190964381, Sync: 0.0, Percep: 0.682230735818545 | Fake: 0.704184333483378, Real: 0.6820155580838522: : 11it [00:08,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11494726190964381, Sync: 0.0, Percep: 0.682230735818545 | Fake: 0.704184333483378, Real: 0.6820155580838522: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11552827690656368, Sync: 0.0, Percep: 0.6822964319816003 | Fake: 0.7041172201816852, Real: 0.6820147496003371: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11552827690656368, Sync: 0.0, Percep: 0.6822964319816003 | Fake: 0.7041172201816852, Real: 0.6820147496003371: : 13it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11771383934787341, Sync: 0.0, Percep: 0.6823638847896031 | Fake: 0.704048331294741, Real: 0.6819869450160435: : 13it [00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11771383934787341, Sync: 0.0, Percep: 0.6823638847896031 | Fake: 0.704048331294741, Real: 0.6819869450160435: : 14it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11777033309141795, Sync: 0.0, Percep: 0.6824390649795532 | Fake: 0.7039715727170308, Real: 0.6820672790209452: : 14it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11777033309141795, Sync: 0.0, Percep: 0.6824390649795532 | Fake: 0.7039715727170308, Real: 0.6820672790209452: : 15it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11740650003775954, Sync: 0.0, Percep: 0.6825169809162617 | Fake: 0.7038920372724533, Real: 0.6821344755589962: : 15it [00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11740650003775954, Sync: 0.0, Percep: 0.6825169809162617 | Fake: 0.7038920372724533, Real: 0.6821344755589962: : 16it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11692967528806013, Sync: 0.0, Percep: 0.6825898394865149 | Fake: 0.7038176690830904, Real: 0.6822101859485402: : 16it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11692967528806013, Sync: 0.0, Percep: 0.6825898394865149 | Fake: 0.7038176690830904, Real: 0.6822101859485402: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11692967528806013, Sync: 0.0, Percep: 0.6825898394865149 | Fake: 0.7038176690830904, Real: 0.6822101859485402: : 17it [00:10,  1.56it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 4\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1002381443977356, Sync: 0.0, Percep: 0.6838125586509705 | Fake: 0.7025697827339172, Real: 0.682578980922699: : 0it [00:02, ?it/s]#015L1: 0.1002381443977356, Sync: 0.0, Percep: 0.6838125586509705 | Fake: 0.7025697827339172, Real: 0.682578980922699: : 1it [00:02,  2.12s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.10821161046624184, Sync: 0.0, Percep: 0.6838854253292084 | Fake: 0.7024955451488495, Real: 0.6826620399951935: : 1it [00:02,  2.12s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.10821161046624184, Sync: 0.0, Percep: 0.6838854253292084 | Fake: 0.7024955451488495, Real: 0.6826620399951935: : 2it [00:02,  1.20s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.12748777121305466, Sync: 0.0, Percep: 0.683954914410909 | Fake: 0.7024247447649637, Real: 0.6821905175844828: : 2it [00:03,  1.20s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.12748777121305466, Sync: 0.0, Percep: 0.683954914410909 | Fake: 0.7024247447649637, Real: 0.6821905175844828: : 3it [00:03,  1.10it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11906721442937851, Sync: 0.0, Percep: 0.6840763837099075 | Fake: 0.7023010551929474, Real: 0.6823777258396149: : 3it [00:03,  1.10it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11906721442937851, Sync: 0.0, Percep: 0.6840763837099075 | Fake: 0.7023010551929474, Real: 0.6823777258396149: : 4it [00:03,  1.30it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1228071004152298, Sync: 0.0, Percep: 0.6841927170753479 | Fake: 0.7021826386451722, Real: 0.6821861267089844: : 4it [00:04,  1.30it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1228071004152298, Sync: 0.0, Percep: 0.6841927170753479 | Fake: 0.7021826386451722, Real: 0.6821861267089844: : 5it [00:04,  1.44it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1248050332069397, Sync: 0.0, Percep: 0.6842924455801646 | Fake: 0.7020811637242635, Real: 0.6820650398731232: : 5it [00:04,  1.44it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1248050332069397, Sync: 0.0, Percep: 0.6842924455801646 | Fake: 0.7020811637242635, Real: 0.6820650398731232: : 6it [00:04,  1.54it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13054231873580388, Sync: 0.0, Percep: 0.6844154681478228 | Fake: 0.7019560251917157, Real: 0.6809912579400199: : 6it [00:05,  1.54it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13054231873580388, Sync: 0.0, Percep: 0.6844154681478228 | Fake: 0.7019560251917157, Real: 0.6809912579400199: : 7it [00:05,  1.61it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12753498367965221, Sync: 0.0, Percep: 0.6846003085374832 | Fake: 0.7017681673169136, Real: 0.681343674659729: : 7it [00:06,  1.61it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12753498367965221, Sync: 0.0, Percep: 0.6846003085374832 | Fake: 0.7017681673169136, Real: 0.681343674659729: : 8it [00:06,  1.67it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12719728466537264, Sync: 0.0, Percep: 0.6844248572985331 | Fake: 0.7019469804233975, Real: 0.6795798076523675: : 8it [00:06,  1.67it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12719728466537264, Sync: 0.0, Percep: 0.6844248572985331 | Fake: 0.7019469804233975, Real: 0.6795798076523675: : 9it [00:06,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12388371750712394, Sync: 0.0, Percep: 0.6846861362457275 | Fake: 0.7016817033290863, Real: 0.6798176825046539: : 9it [00:07,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12388371750712394, Sync: 0.0, Percep: 0.6846861362457275 | Fake: 0.7016817033290863, Real: 0.6798176825046539: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1215962374752218, Sync: 0.0, Percep: 0.6847382404587485 | Fake: 0.7016286253929138, Real: 0.6767894354733553: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1215962374752218, Sync: 0.0, Percep: 0.6847382404587485 | Fake: 0.7016286253929138, Real: 0.6767894354733553: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11944874562323093, Sync: 0.0, Percep: 0.6816867043574651 | Fake: 0.7048572699228922, Real: 0.6696194211641947: : 11it [00:08,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11944874562323093, Sync: 0.0, Percep: 0.6816867043574651 | Fake: 0.7048572699228922, Real: 0.6696194211641947: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12188522689617597, Sync: 0.0, Percep: 0.6823766414935772 | Fake: 0.7041486134895911, Real: 0.6375890374183655: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12188522689617597, Sync: 0.0, Percep: 0.6823766414935772 | Fake: 0.7041486134895911, Real: 0.6375890374183655: : 13it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12242120344723974, Sync: 0.0, Percep: 0.6830961321081434 | Fake: 0.7034126818180084, Real: 0.5923400385571378: : 13it [00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12242120344723974, Sync: 0.0, Percep: 0.6830961321081434 | Fake: 0.7034126818180084, Real: 0.5923400385571378: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.120870640873909, Sync: 0.0, Percep: 0.6666781584421794 | Fake: 0.7316075523694356, Real: 0.552854499155607: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.120870640873909, Sync: 0.0, Percep: 0.6666781584421794 | Fake: 0.7316075523694356, Real: 0.552854499155607: : 15it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12338440306484699, Sync: 0.0, Percep: 0.671539481729269 | Fake: 0.7261533439159393, Real: 0.5253891730069427: : 15it [00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12338440306484699, Sync: 0.0, Percep: 0.671539481729269 | Fake: 0.7261533439159393, Real: 0.5253891730069427: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12512506982859442, Sync: 0.0, Percep: 0.695420948898091 | Fake: 0.70792316513903, Real: 0.49453410247459784: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12512506982859442, Sync: 0.0, Percep: 0.695420948898091 | Fake: 0.70792316513903, Real: 0.49453410247459784: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12512506982859442, Sync: 0.0, Percep: 0.695420948898091 | Fake: 0.70792316513903, Real: 0.49453410247459784: : 17it [00:10,  1.55it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 5\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14709579944610596, Sync: 0.0, Percep: 7.289750576019287 | Fake: 0.0006906178314238787, Real: 2.2412188053131104: : 0it [00:01, ?it/s]#015L1: 0.14709579944610596, Sync: 0.0, Percep: 7.289750576019287 | Fake: 0.0006906178314238787, Real: 2.2412188053131104: : 1it [00:01,  1.75s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.14163386076688766, Sync: 0.0, Percep: 4.012343347072601 | Fake: 0.3268670333782211, Real: 1.121565380715765: : 1it [00:02,  1.75s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.14163386076688766, Sync: 0.0, Percep: 4.012343347072601 | Fake: 0.3268670333782211, Real: 1.121565380715765: : 2it [00:02,  1.05s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1431437830130259, Sync: 0.0, Percep: 2.6925260747472444 | Fake: 6.587348255406444, Real: 0.74771025381051: : 2it [00:02,  1.05s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1431437830130259, Sync: 0.0, Percep: 2.6925260747472444 | Fake: 6.587348255406444, Real: 0.74771025381051: : 3it [00:02,  1.20it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1390657238662243, Sync: 0.0, Percep: 2.1952280048280954 | Fake: 5.11127703852253, Real: 0.723672334861476: : 3it [00:03,  1.20it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1390657238662243, Sync: 0.0, Percep: 2.1952280048280954 | Fake: 5.11127703852253, Real: 0.723672334861476: : 4it [00:03,  1.38it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1330311641097069, Sync: 0.0, Percep: 1.8965805605053903 | Fake: 4.225897856289521, Real: 0.7192558205220848: : 4it [00:04,  1.38it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1330311641097069, Sync: 0.0, Percep: 1.8965805605053903 | Fake: 4.225897856289521, Real: 0.7192558205220848: : 5it [00:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13436677927772203, Sync: 0.0, Percep: 1.6970791084071 | Fake: 3.636042164211782, Real: 0.7096627185819671: : 5it [00:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13436677927772203, Sync: 0.0, Percep: 1.6970791084071 | Fake: 3.636042164211782, Real: 0.7096627185819671: : 6it [00:04,  1.59it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13308574578591756, Sync: 0.0, Percep: 1.555328527731555 | Fake: 3.2139796158631464, Real: 0.7067764937944178: : 6it [00:05,  1.59it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13308574578591756, Sync: 0.0, Percep: 1.555328527731555 | Fake: 3.2139796158631464, Real: 0.7067764937944178: : 7it [00:05,  1.64it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14003877248615026, Sync: 0.0, Percep: 1.4492704393342137 | Fake: 2.8971842202881817, Real: 0.7026799939631019: : 7it [00:05,  1.64it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.14003877248615026, Sync: 0.0, Percep: 1.4492704393342137 | Fake: 2.8971842202881817, Real: 0.7026799939631019: : 8it [00:05,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13910038686460918, Sync: 0.0, Percep: 1.3677810538146231 | Fake: 2.6498231996730386, Real: 0.7034837954625901: : 8it [00:06,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13910038686460918, Sync: 0.0, Percep: 1.3677810538146231 | Fake: 2.6498231996730386, Real: 0.7034837954625901: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13829228058457374, Sync: 0.0, Percep: 1.3031625665724278 | Fake: 2.45138956096489, Real: 0.7033353167818859: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13829228058457374, Sync: 0.0, Percep: 1.3031625665724278 | Fake: 2.45138956096489, Real: 0.7033353167818859: : 10it [00:06,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.133373116905039, Sync: 0.0, Percep: 1.2503858784382993 | Fake: 2.288947054714134, Real: 0.7047130644363775: : 10it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.133373116905039, Sync: 0.0, Percep: 1.2503858784382993 | Fake: 2.288947054714134, Real: 0.7047130644363775: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13157396328945956, Sync: 0.0, Percep: 1.2035701653609674 | Fake: 2.1563454103694917, Real: 0.7016590749262832: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.13157396328945956, Sync: 0.0, Percep: 1.2035701653609674 | Fake: 2.1563454103694917, Real: 0.7016590749262832: : 12it [00:07,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1310969000825515, Sync: 0.0, Percep: 1.1676131148750966 | Fake: 2.0406225857575637, Real: 0.7023307969972777: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1310969000825515, Sync: 0.0, Percep: 1.1676131148750966 | Fake: 2.0406225857575637, Real: 0.7023307969972777: : 13it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1300846588398729, Sync: 0.0, Percep: 1.137519541063479 | Fake: 1.9407702498969488, Real: 0.7047059037888955: : 13it [00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1300846588398729, Sync: 0.0, Percep: 1.137519541063479 | Fake: 1.9407702498969488, Real: 0.7047059037888955: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12773733536402385, Sync: 0.0, Percep: 1.1123438462615014 | Fake: 1.8534277742883811, Real: 0.7078029604783903: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12773733536402385, Sync: 0.0, Percep: 1.1123438462615014 | Fake: 1.8534277742883811, Real: 0.7078029604783903: : 15it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12650255812332034, Sync: 0.0, Percep: 1.090846958104521 | Fake: 1.7765401029755594, Real: 0.7104065317689674: : 15it [00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12650255812332034, Sync: 0.0, Percep: 1.090846958104521 | Fake: 1.7765401029755594, Real: 0.7104065317689674: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12498613359297023, Sync: 0.0, Percep: 1.072991103372153 | Fake: 1.7077577347072828, Real: 0.7137892291853752: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12498613359297023, Sync: 0.0, Percep: 1.072991103372153 | Fake: 1.7077577347072828, Real: 0.7137892291853752: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12498613359297023, Sync: 0.0, Percep: 1.072991103372153 | Fake: 1.7077577347072828, Real: 0.7137892291853752: : 17it [00:10,  1.60it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 6\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09642909467220306, Sync: 0.0, Percep: 0.794322669506073 | Fake: 0.6013534665107727, Real: 0.7890404462814331: : 0it [00:02, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09642909467220306, Sync: 0.0, Percep: 0.794322669506073 | Fake: 0.6013534665107727, Real: 0.7890404462814331: : 1it [00:02,  2.04s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11103692650794983, Sync: 0.0, Percep: 0.7755641043186188 | Fake: 0.6173609793186188, Real: 0.7609387934207916: : 1it [00:02,  2.04s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11103692650794983, Sync: 0.0, Percep: 0.7755641043186188 | Fake: 0.6173609793186188, Real: 0.7609387934207916: : 2it [00:02,  1.16s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11229146768649419, Sync: 0.0, Percep: 0.7667289574941 | Fake: 0.6249832510948181, Real: 0.7274206678072611: : 2it [00:03,  1.16s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11229146768649419, Sync: 0.0, Percep: 0.7667289574941 | Fake: 0.6249832510948181, Real: 0.7274206678072611: : 3it [00:03,  1.12it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10760736092925072, Sync: 0.0, Percep: 0.6954066976904869 | Fake: 0.7094035893678665, Real: 0.6594429537653923: : 3it [00:03,  1.12it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10760736092925072, Sync: 0.0, Percep: 0.6954066976904869 | Fake: 0.7094035893678665, Real: 0.6594429537653923: : 4it [00:03,  1.32it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10629079788923264, Sync: 0.0, Percep: 0.731615549325943 | Fake: 0.6752648830413819, Real: 0.6794092953205109: : 4it [00:04,  1.32it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10629079788923264, Sync: 0.0, Percep: 0.731615549325943 | Fake: 0.6752648830413819, Real: 0.6794092953205109: : 5it [00:04,  1.45it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10395549610257149, Sync: 0.0, Percep: 0.7516836573680242 | Fake: 0.6554530958334605, Real: 0.6827496836582819: : 5it [00:04,  1.45it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10395549610257149, Sync: 0.0, Percep: 0.7516836573680242 | Fake: 0.6554530958334605, Real: 0.6827496836582819: : 6it [00:04,  1.56it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10546430413212095, Sync: 0.0, Percep: 0.7520292954785484 | Fake: 0.6527864251817975, Real: 0.6722687610558101: : 6it [00:05,  1.56it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10546430413212095, Sync: 0.0, Percep: 0.7520292954785484 | Fake: 0.6527864251817975, Real: 0.6722687610558101: : 7it [00:05,  1.62it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11081246566027403, Sync: 0.0, Percep: 0.9529877118766308 | Fake: 0.5837444579228759, Real: 0.7251244597136974: : 7it [00:05,  1.62it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11081246566027403, Sync: 0.0, Percep: 0.9529877118766308 | Fake: 0.5837444579228759, Real: 0.7251244597136974: : 8it [00:05,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11370020690891477, Sync: 0.0, Percep: 0.9693401356538137 | Fake: 0.5639017447829247, Real: 0.7409890128506554: : 8it [00:06,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11370020690891477, Sync: 0.0, Percep: 0.9693401356538137 | Fake: 0.5639017447829247, Real: 0.7409890128506554: : 9it [00:06,  1.70it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11616801843047142, Sync: 0.0, Percep: 0.9640038877725601 | Fake: 0.5586274348199367, Real: 0.7220046311616898: : 9it [00:07,  1.70it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11616801843047142, Sync: 0.0, Percep: 0.9640038877725601 | Fake: 0.5586274348199367, Real: 0.7220046311616898: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1179307448593053, Sync: 0.0, Percep: 0.9596670513803308 | Fake: 0.5543112233281136, Real: 0.691618640314449: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1179307448593053, Sync: 0.0, Percep: 0.9596670513803308 | Fake: 0.5543112233281136, Real: 0.691618640314449: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11683947468797366, Sync: 0.0, Percep: 0.9374627992510796 | Fake: 0.5674641150981188, Real: 0.6530496540168921: : 11it [00:08,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11683947468797366, Sync: 0.0, Percep: 0.9374627992510796 | Fake: 0.5674641150981188, Real: 0.6530496540168921: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11644061196308869, Sync: 0.0, Percep: 0.9868108515556042 | Fake: 0.5418118381729493, Real: 0.688845585171993: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11644061196308869, Sync: 0.0, Percep: 0.9868108515556042 | Fake: 0.5418118381729493, Real: 0.688845585171993: : 13it [00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11638074527893748, Sync: 0.0, Percep: 0.9941387282950538 | Fake: 0.5324350579508713, Real: 0.6808119171432087: : 13it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11638074527893748, Sync: 0.0, Percep: 0.9941387282950538 | Fake: 0.5324350579508713, Real: 0.6808119171432087: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1182083452741305, Sync: 0.0, Percep: 0.9904831786950429 | Fake: 0.5300104344884554, Real: 0.6496763239304225: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1182083452741305, Sync: 0.0, Percep: 0.9904831786950429 | Fake: 0.5300104344884554, Real: 0.6496763239304225: : 15it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1172419497743249, Sync: 0.0, Percep: 0.9896859433501959 | Fake: 0.5264101871289313, Real: 0.6403240943327546: : 15it [00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1172419497743249, Sync: 0.0, Percep: 0.9896859433501959 | Fake: 0.5264101871289313, Real: 0.6403240943327546: : 16it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11707344958010842, Sync: 0.0, Percep: 0.9894758375251994 | Fake: 0.5229164155967095, Real: 0.6275550363694921: : 16it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11707344958010842, Sync: 0.0, Percep: 0.9894758375251994 | Fake: 0.5229164155967095, Real: 0.6275550363694921: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11707344958010842, Sync: 0.0, Percep: 0.9894758375251994 | Fake: 0.5229164155967095, Real: 0.6275550363694921: : 17it [00:10,  1.57it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 7\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10880760848522186, Sync: 0.0, Percep: 1.1280573606491089 | Fake: 0.3925306499004364, Real: 0.2845029830932617: : 0it [00:01, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10880760848522186, Sync: 0.0, Percep: 1.1280573606491089 | Fake: 0.3925306499004364, Real: 0.2845029830932617: : 1it [00:01,  1.97s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.10144945234060287, Sync: 0.0, Percep: 1.2613893747329712 | Fake: 0.3398088365793228, Real: 0.49619895219802856: : 1it [00:02,  1.97s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.10144945234060287, Sync: 0.0, Percep: 1.2613893747329712 | Fake: 0.3398088365793228, Real: 0.49619895219802856: : 2it [00:02,  1.14s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11888856689135234, Sync: 0.0, Percep: 1.2593683004379272 | Fake: 0.33846063415209454, Real: 0.3838846335808436: : 2it [00:03,  1.14s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.11888856689135234, Sync: 0.0, Percep: 1.2593683004379272 | Fake: 0.33846063415209454, Real: 0.3838846335808436: : 3it [00:03,  1.14it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11689082160592079, Sync: 0.0, Percep: 1.261348694562912 | Fake: 0.3366064205765724, Real: 0.2879134751856327: : 3it [00:03,  1.14it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11689082160592079, Sync: 0.0, Percep: 1.261348694562912 | Fake: 0.3366064205765724, Real: 0.2879134751856327: : 4it [00:03,  1.33it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11871748268604279, Sync: 0.0, Percep: 1.3961868047714234 | Fake: 0.30067082941532136, Real: 0.23033078014850616: : 4it [00:04,  1.33it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11871748268604279, Sync: 0.0, Percep: 1.3961868047714234 | Fake: 0.30067082941532136, Real: 0.23033078014850616: : 5it [00:04,  1.46it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11403614779313405, Sync: 0.0, Percep: 1.8181976278622944 | Fake: 0.25415992240111035, Real: 0.5089840317765871: : 5it [00:04,  1.46it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11403614779313405, Sync: 0.0, Percep: 1.8181976278622944 | Fake: 0.25415992240111035, Real: 0.5089840317765871: : 6it [00:04,  1.56it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11479282272713524, Sync: 0.0, Percep: 1.5584551095962524 | Fake: 14.50356564777238, Real: 0.4362720272370747: : 6it [00:05,  1.56it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11479282272713524, Sync: 0.0, Percep: 1.5584551095962524 | Fake: 14.50356564777238, Real: 0.4362720272370747: : 7it [00:05,  1.63it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11476319562643766, Sync: 0.0, Percep: 1.3636482208967209 | Fake: 25.190619941800833, Real: 0.3817380238324404: : 7it [00:05,  1.63it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11476319562643766, Sync: 0.0, Percep: 1.3636482208967209 | Fake: 25.190619941800833, Real: 0.3817380238324404: : 8it [00:05,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11546332554684745, Sync: 0.0, Percep: 1.2121317519081964 | Fake: 33.50277328160074, Real: 0.3393226878510581: : 8it [00:06,  1.68it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11546332554684745, Sync: 0.0, Percep: 1.2121317519081964 | Fake: 33.50277328160074, Real: 0.3393226878510581: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11616664826869964, Sync: 0.0, Percep: 1.0909185767173768 | Fake: 40.15249595344066, Real: 0.3053904190659523: : 9it [00:07,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11616664826869964, Sync: 0.0, Percep: 1.0909185767173768 | Fake: 40.15249595344066, Real: 0.3053904190659523: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11563718996264717, Sync: 0.0, Percep: 0.9917441606521606 | Fake: 45.59317813949151, Real: 0.27762765369632025: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11563718996264717, Sync: 0.0, Percep: 0.9917441606521606 | Fake: 45.59317813949151, Real: 0.27762765369632025: : 11it [00:07,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11826061829924583, Sync: 0.0, Percep: 0.9090988139311472 | Fake: 50.12707996120056, Real: 0.25449201588829357: : 11it [00:08,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11826061829924583, Sync: 0.0, Percep: 0.9090988139311472 | Fake: 50.12707996120056, Real: 0.25449201588829357: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11913146880956796, Sync: 0.0, Percep: 0.8391681359364436 | Fake: 53.96345842572359, Real: 0.23491570697380945: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11913146880956796, Sync: 0.0, Percep: 0.8391681359364436 | Fake: 53.96345842572359, Real: 0.23491570697380945: : 13it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11736470407673291, Sync: 0.0, Percep: 0.7792275547981262 | Fake: 57.25178282388619, Real: 0.21813601361853735: : 13it [00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11736470407673291, Sync: 0.0, Percep: 0.7792275547981262 | Fake: 57.25178282388619, Real: 0.21813601361853735: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1163832833369573, Sync: 0.0, Percep: 0.7272790511449178 | Fake: 60.101663968960445, Real: 0.20359361271063486: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1163832833369573, Sync: 0.0, Percep: 0.7272790511449178 | Fake: 60.101663968960445, Real: 0.20359361271063486: : 15it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11597764678299427, Sync: 0.0, Percep: 0.6818241104483604 | Fake: 62.595309970900416, Real: 0.1908690119162202: : 15it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11597764678299427, Sync: 0.0, Percep: 0.6818241104483604 | Fake: 62.595309970900416, Real: 0.1908690119162202: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11593756561770159, Sync: 0.0, Percep: 0.641716809833751 | Fake: 64.7955858549651, Real: 0.17964142297997193: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11593756561770159, Sync: 0.0, Percep: 0.641716809833751 | Fake: 64.7955858549651, Real: 0.17964142297997193: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.11593756561770159, Sync: 0.0, Percep: 0.641716809833751 | Fake: 64.7955858549651, Real: 0.17964142297997193: : 17it [00:10,  1.58it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 8\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.08560869097709656, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 0it [00:02, ?it/s]#015L1: 0.08560869097709656, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 1it [00:02,  2.04s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.09321236237883568, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 1it [00:02,  2.04s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.09321236237883568, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 2it [00:02,  1.17s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.0970798134803772, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 2it [00:03,  1.17s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.0970798134803772, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 3it [00:03,  1.12it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09428287856280804, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 3it [00:03,  1.12it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09428287856280804, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 4it [00:03,  1.33it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09168311208486557, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 4it [00:04,  1.33it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09168311208486557, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 5it [00:04,  1.46it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09822606171170871, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 5it [00:04,  1.46it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09822606171170871, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 6it [00:04,  1.56it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09524619792188917, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 6it [00:05,  1.56it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09524619792188917, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 7it [00:05,  1.63it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09673240315169096, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 7it [00:05,  1.63it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09673240315169096, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 8it [00:05,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09721708380513722, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 8it [00:06,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09721708380513722, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 9it [00:06,  1.70it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09700060412287712, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 9it [00:07,  1.70it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09700060412287712, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09992983598600734, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 10it [00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09992983598600734, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 11it [00:07,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10063347841302554, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 11it [00:08,  1.74it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10063347841302554, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10049045830965042, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 12it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10049045830965042, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 13it [00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09877316547291619, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 13it [00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09877316547291619, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 14it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09880463927984237, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 14it [00:09,  1.78it/s]#015L1: 0.09880463927984237, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 15it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09831129992380738, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 15it [00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09831129992380738, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09942051899783752, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09942051899783752, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 17it [00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09942051899783752, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 17it [00:10,  1.57it/s]\u001b[0m\n",
      "\u001b[34mStarting Epoch: 9\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.12335935980081558, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 0it [00:01, ?it/s]#015L1: 0.12335935980081558, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 1it [00:01,  1.84s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1062772162258625, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 1it [00:02,  1.84s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.1062772162258625, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 2it [00:02,  1.08s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.10133711248636246, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 2it [00:02,  1.08s/it]\u001b[0m\n",
      "\u001b[34mL1: 0.10133711248636246, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 3it [00:02,  1.18it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10136769898235798, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 3it [00:03,  1.18it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10136769898235798, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 4it [00:03,  1.37it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09786392003297806, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 4it [00:04,  1.37it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09786392003297806, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 5it [00:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1039934145907561, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 5it [00:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1039934145907561, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 6it [00:04,  1.59it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10187032499483653, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 6it [00:05,  1.59it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10187032499483653, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 7it [00:05,  1.64it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10082958359271288, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 7it [00:05,  1.64it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10082958359271288, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 8it [00:05,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10049924668338564, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 8it [00:06,  1.69it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10049924668338564, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09952621385455132, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 9it [00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09952621385455132, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 10it [00:06,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.0994572483680465, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 10it [00:07,  1.75it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.0994572483680465, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 11it [00:07,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09989212080836296, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 11it [00:07,  1.76it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.09989212080836296, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 12it [00:07,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10259272043521588, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 12it [00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10259272043521588, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 13it [00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1038839758506843, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 13it [00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.1038839758506843, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 14it [00:09,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10343822538852691, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 14it [00:09,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10343822538852691, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 15it [00:09,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10361767560243607, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 15it [00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10361767560243607, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10411935562596601, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 16it [00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10411935562596601, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 17it [00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[34mL1: 0.10411935562596601, Sync: 0.0, Percep: 0.0 | Fake: 100.0, Real: 0.0: : 17it [00:10,  1.60it/s]\u001b[0m\n",
      "\u001b[34mfinished wav2lip\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/0_3.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/0_3.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/1_1.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/1_1.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/0_4.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/0_4.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/1_2.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/1_2.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/3_0.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/3_0.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/2_3.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/2_3.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/2_2.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/2_2.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/0_0.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/0_0.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/1_4.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/1_4.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/3_4.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/3_4.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/1_0.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/1_0.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/3_1.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/3_1.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/3_3.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/3_3.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/1_3.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/1_3.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/3_2.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/3_2.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/2_1.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/2_1.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/2_0.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/2_0.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/0_2.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/0_2.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/0_1.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/0_1.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/samples_step000000000/2_4.jpg s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/samples_step000000000/2_4.jpg\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/disc_checkpoint_step000000001.pth s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/disc_checkpoint_step000000001.pth\u001b[0m\n",
      "\u001b[34mcp /tmp/trained_wav2lip_288x288/checkpoint_step000000001.pth s3://sagemaker-us-west-2-687912291502/wav2lip_288x288/output/2023-10-21-08-22-12/checkpoint_step000000001.pth\u001b[0m\n",
      "\u001b[34m2023-10-21 08:22:15,179 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-21 08:22:15,179 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-21 08:22:15,180 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-21 08:23:14 Uploading - Uploading generated training model\n",
      "2023-10-21 08:23:14 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 603\n",
      "Billable seconds: 603\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'wav2lip-288x288-demo'         \n",
    "\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train-distribute.sh',\n",
    "                      source_dir='./wav2lip_288x288/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      keep_alive_period_in_seconds=3600,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e673c",
   "metadata": {
    "tags": []
   },
   "source": [
    "You could find the model path in S3 from above logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54873d2f-2b86-41f9-ae2d-f71693d92cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
