{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4416c96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An sample to finetune wave2lip on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95febd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158e94ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0c829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## download training script from github\n",
    "!rm -rf ./wav2lip_288x288\n",
    "!git clone https://github.com/whn09/wav2lip_288x288.git\n",
    "!cp ./s5cmd ./wav2lip_288x288/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5a450",
   "metadata": {},
   "source": [
    "## Download pretrained model(expert Discriminator & face detect) and upload to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e0bed-ee4f-43d5-afbb-c61907353552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ./wav2lip_288x288/face_detection/detection/sfd/ && wget https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n",
    "!mv ./wav2lip_288x288/face_detection/detection/sfd/s3fd-619a316812.pth ./wav2lip_288x288/face_detection/detection/sfd/s3fd.pth\n",
    "# !cd ./Wav2Lip/models && wget https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d498",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2057f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN  apt-get update\n",
    "RUN  echo \"Y\"|apt-get install ffmpeg\n",
    "RUN  pip3 install deepspeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ee553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b0fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53800617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-wav2lip-288x288-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d98c7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa7288-4f61-4896-93b7-12fd03055c53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Prepare train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef55e2",
   "metadata": {},
   "source": [
    "use s3 dataset path, which is aligned with data_root of wav2lip \n",
    "should be like:  \n",
    "data_root (mvlrs_v1)  \n",
    "├── main, pretrain (we use only main folder in this work)  \n",
    "|\t├── list of folders  \n",
    "|\t│   ├── five-digit numbered video IDs ending with (.mp4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90becfa3-2ec1-4735-b337-33891f12d1d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!wget https://bj.bcebos.com/ai-studio-online/88f38e14dc9f4893bdb3ad6857b810a9e0ed6d63f4f84d7da0b69e165ca4f7a5?authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2022-09-04T15%3A27%3A21Z%2F-1%2F%2F2048bfe72ab62c84e2a6622f565ac7cc30830ffdace4607bf847909d1038b5b0&responseContentDisposition=attachment%3B%20filename%3Dmain.zip\n",
    "!./s5cmd sync ./main2/ s3://{sagemaker_default_bucket}/wav2lip_288x288/train_data/main2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7f140-733f-4626-97af-78b3436c8616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### 准备filelists/train.txt\n",
    "import os\n",
    "\n",
    "import time\n",
    "from glob import glob\n",
    "import shutil,os\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# 去除名字的特殊符号，统一序号视频文件命名\n",
    " \n",
    "# def original_video_name_format():\n",
    "#     base_path = \"../LSR2/main\"\n",
    "#     result = list(glob(\"{}/*\".format(base_path),recursive=False))\n",
    "#     file_num = 0\n",
    "#     result_list = []\n",
    " \n",
    "#     for each in result:\n",
    "#         file_num +=1\n",
    "#         new_position =\"{0}{1}\".format( int(time.time()),file_num)\n",
    "#         result_list.append(new_position)\n",
    "#         shutil.move(each, os.path.join(base_path,new_position+\".mp4\"))\n",
    "#         pass\n",
    "\n",
    "def trained_data_name_format():\n",
    "    base_path = \"../LSR2/lrs2_preprocessed\"\n",
    "    # result = list(glob(\"{}/*\".format(base_path)))\n",
    "    result = os.listdir(base_path)\n",
    "    print(result)\n",
    "    result_list = []\n",
    "    for i,dirpath in enumerate(result):\n",
    "        # shutil.move(dirpath,\"{0}/{1}\".format(base_path,i))\n",
    "        # result_list.append(str(i))\n",
    "        # print('dirpath:', dirpath)\n",
    "        result_list.append(dirpath)\n",
    "    if len(result_list)<14:\n",
    "        test_result=val_result=train_result=result_list\n",
    "    else:\n",
    "        train_result,test_result = train_test_split(result_list,test_size=0.15, random_state=42)\n",
    "        test_result, val_result = train_test_split(test_result, test_size=0.5, random_state=42)\n",
    " \n",
    "    for file_name,dataset in zip((\"train.txt\",\"test.txt\",\"val.txt\"),(train_result,test_result,val_result)):\n",
    "        with open(os.path.join(\"filelists\",file_name),'w',encoding='utf-8') as fi:\n",
    "            for dataset_i in dataset:\n",
    "                # print('dataset_i:', dataset_i)\n",
    "                video_result = os.listdir(os.path.join(base_path, dirpath))\n",
    "                # print('video_result:', video_result)\n",
    "                video_result = [dataset_i+'/'+video for video in video_result]\n",
    "                fi.write(\"\\n\".join(video_result))\n",
    "                fi.write(\"\\n\")\n",
    " \n",
    "    # print(\"\\n\".join(result_list))\n",
    "\n",
    "trained_data_name_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8ac60-b8bd-44b6-9308-f024ea49c040",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### for local data process test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f553a9-582a-43a5-9dd9-6742c390c02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a618b1d-5bf1-492a-92e0-a9043c117da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###data process\n",
    "#!pip install -r ./requirements.txt\n",
    "!cd ./Wav2Lip/ && python preprocess.py --data_root ../main2 --preprocessed_root ../lrs2_preprocessed2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62c4f7-5beb-4c14-be0c-29ed079bbd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./s5cmd sync ./lrs2_preprocessed2/ s3://{sagemaker_default_bucket}/288x288/train_data/lrs2_preprocessed/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d5b8f-188e-4e46-b998-903b462f59cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 开始训练(单机单卡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cedcb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./wav2lip_288x288/train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://${sagemaker_default_bucket}/wav2lip_288x288/train_data/main2/* /tmp/main2/\n",
    "pip install -r ./requirements.txt\n",
    "\n",
    "echo \"--------preprocess wav-----------\"\n",
    "python ./preprocess.py --data_root /tmp/main2 --preprocessed_root /tmp/lrs2_preprocessed2/\n",
    "echo \"finished preprocess\"\n",
    "\n",
    "echo \"--------prepare tain list--------\"\n",
    "python ./generate_filelists.py --data_root /tmp/lrs2_preprocessed2/\n",
    "echo \"finished train list prepare\"\n",
    "\n",
    "echo \"--------train the expert discriminator------\"\n",
    "startdt=$(date +%s)\n",
    "python ./color_syncnet_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_syncnet/\n",
    "enddt=$(date +%s)\n",
    "interval_minutes=$(( (enddt - startdt) ))\n",
    "echo \"时间间隔为 $interval_minutes 秒\"\n",
    "echo \"finished train syncnet\"\n",
    "\n",
    "\n",
    "echo \"--------train wav2lip-----------------------\" \n",
    "python ./hq_wav2lip_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_wav2lip_288x288/ --syncnet_checkpoint_path /tmp/trained_syncnet/checkpoint_step000000001.pth\n",
    "echo \"finished wav2lip\"\n",
    "\n",
    "./s5cmd sync /tmp/trained_wav2lip_288x288/ s3://${sagemaker_default_bucket}/models/wav2lip_288x288/output/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "\n",
    "###inference\n",
    "#echo \"begin inference\"\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/face_video/* /tmp/face_video/\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/audio/* /tmp/audio/\n",
    "#python ./inference.py --checkpoint_path /tmp/trained_wav2lip_288x288/checkpoint_step000000001.pth  --face /tmp/face_video/VID20230623143819.mp4 --audio /tmp/audio/测试wav2lip.mp3\n",
    "#./s5cmd sync ./results/result_voice.mp4  s3://${sagemaker_default_bucket}/models/wav2lip_288x288/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517889b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad795754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/wav2lip_288x288/train_data/'.format(sagemaker_default_bucket)\n",
    "inputs = {'data_root': train_data_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab36100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'sagemaker_default_bucket': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'wav2lip-288x288-demo'         \n",
    "\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train.sh',\n",
    "                      source_dir='./wav2lip_288x288/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      keep_alive_period_in_seconds=3600,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa5918-57f2-455e-98f3-ecb59c47135f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://${sagemaker_default_bucket}/models/wav2lip_288x288/results/result_voice.mp4 ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d6593-90ad-4409-ae65-17e618a6c91c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### deepspeed （单机多卡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420efdac-3199-40e2-a81a-1c727ca7c868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-wav2lip-288x288-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0e5783-8d3e-4964-bcdd-bc3b96fe8c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-wav2lip-288x288-demo:latest'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f960df78-8a29-4991-955d-5c04c82dd1dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/wav2lip_288x288/train_data/'.format(sagemaker_default_bucket)\n",
    "inputs = {'data_root': train_data_path}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb8f1d-2d67-48b7-9e3a-e930246c4fde",
   "metadata": {
    "tags": []
   },
   "source": [
    "### sycnet 模型deepspeed改造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae788bea-5ef2-4a47-849b-60fee3caa59e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/color_syncnet_dist_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/color_syncnet_dist_train.py\n",
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "import audio\n",
    "import deepspeed\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# 忽略所有警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the expert lip-sync discriminator')\n",
    "\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True)\n",
    "\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\n",
    "parser.add_argument('--checkpoint_path', help='Resumed from this checkpoint', default=None, type=str)\n",
    "# -------------------------------- multi gpus --------------------------------------------\n",
    "parser.add_argument('--local_rank', type=int, default=-1, help='local rank passed from distributed launcher')\n",
    "parser = deepspeed.add_config_arguments(parser)\n",
    "# -------------------------------- multi gpus --------------------------------------------\n",
    "args = parser.parse_args()\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "best_loss = 1000\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "\tdef __init__(self, split):\n",
    "\t\tself.all_videos = get_image_list(args.data_root, split)\n",
    "\n",
    "\tdef get_frame_id(self, frame):\n",
    "\t\treturn int(basename(frame).split('.')[0])\n",
    "\n",
    "\tdef get_window(self, start_frame):\n",
    "\t\tstart_id = self.get_frame_id(start_frame)\n",
    "\t\tvidname = dirname(start_frame)\n",
    "\n",
    "\t\twindow_fnames = []\n",
    "\t\tfor frame_id in range(start_id, start_id + syncnet_T):\n",
    "\t\t\t# frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "\t\t\tframe = join(vidname, f'{frame_id:05}.jpg')\n",
    "\t\t\tif not isfile(frame):\n",
    "\t\t\t\treturn None\n",
    "\t\t\twindow_fnames.append(frame)\n",
    "\t\treturn window_fnames\n",
    "\n",
    "\tdef crop_audio_window(self, spec, start_frame):\n",
    "\t\t# num_frames = (T x hop_size * fps) / sample_rate\n",
    "\t\tstart_frame_num = self.get_frame_id(start_frame)\n",
    "\t\tstart_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "\t\tend_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "\t\treturn spec[start_idx: end_idx, :]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.all_videos)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\twhile 1:\n",
    "\t\t\tidx = random.randint(0, len(self.all_videos) - 1)\n",
    "\t\t\tvidname = self.all_videos[idx]\n",
    "\t\t\timg_names = list(glob(join(vidname, '*.jpg')))\n",
    "\t\t\tif len(img_names) <= 3 * syncnet_T:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\timg_name = random.choice(img_names)\n",
    "\t\t\twrong_img_name = random.choice(img_names)\n",
    "\t\t\twhile wrong_img_name == img_name:\n",
    "\t\t\t\twrong_img_name = random.choice(img_names)\n",
    "\t\t\tif random.choice([True, False]):\n",
    "\t\t\t\ty = torch.ones(1).float()\n",
    "\t\t\t\tchosen = img_name\n",
    "\t\t\telse:\n",
    "\t\t\t\ty = torch.zeros(1).float()\n",
    "\t\t\t\tchosen = wrong_img_name\n",
    "\t\t\twindow_fnames = self.get_window(chosen)\n",
    "\t\t\tif window_fnames is None:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\twindow = []\n",
    "\t\t\tall_read = True\n",
    "\t\t\tfor fname in window_fnames:\n",
    "\t\t\t\timg = cv2.imread(fname)\n",
    "\t\t\t\tif img is None:\n",
    "\t\t\t\t\tall_read = False\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\timg = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tall_read = False\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\twindow.append(img)\n",
    "\n",
    "\t\t\tif not all_read: continue\n",
    "\n",
    "\t\t\ttry:\n",
    "\t\t\t\twavpath = join(vidname, \"audio.wav\")\n",
    "\t\t\t\twav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "\t\t\t\torig_mel = audio.melspectrogram(wav).T\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tmel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "\t\t\tif (mel.shape[0] != syncnet_mel_step_size):\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# H x W x 3 * T\n",
    "\t\t\tx = np.concatenate(window, axis=2) / 255.\n",
    "\t\t\tx = x.transpose(2, 0, 1)\n",
    "\t\t\tx = x[:, x.shape[1] // 2:]\n",
    "\n",
    "\t\t\tx = torch.FloatTensor(x)\n",
    "\t\t\tmel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "\t\t\treturn x, mel, y\n",
    "\n",
    "\n",
    "def cosine_loss(a, v, y):\n",
    "\tlogloss = nn.BCELoss()\n",
    "\td = nn.functional.cosine_similarity(a, v)\n",
    "\tloss = logloss(d.unsqueeze(1), y)\n",
    "\treturn loss\n",
    "\n",
    "\n",
    "def train(device, model_engine, train_data_loader, test_data_loader, optimizer_deepspeed,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None, sampler=None):\n",
    "\tglobal global_step, global_epoch\n",
    "\n",
    "\twhile global_epoch < nepochs:\n",
    "\t\tsampler.set_epoch(global_epoch)\n",
    "\t\tst_e = time()\n",
    "\t\trunning_loss = 0.\n",
    "\t\tfor step, (x, mel, y) in enumerate(train_data_loader):\n",
    "\t\t\tst = time()\n",
    "\t\t\tmodel_engine.train()\n",
    "\n",
    "\t\t\t# Transform data to CUDA device\n",
    "\t\t\tx = x.to(device)\n",
    "\t\t\tmel = mel.to(device)\n",
    "\t\t\ta, v = model_engine(mel, x)\n",
    "\t\t\ty = y.to(device)\n",
    "\n",
    "\t\t\tloss = cosine_loss(a, v, y)\n",
    "\t\t\tmodel_engine.backward(loss)\n",
    "\t\t\tmodel_engine.step()\n",
    "\n",
    "\t\t\tglobal_step += 1\n",
    "\t\t\trunning_loss += loss.item()\n",
    "\n",
    "\t\t\tif global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "\t\t\t\tsaved_model_name = join(checkpoint_dir, f\"step{global_step:09d}_{running_loss / (step + 1):.8f}.pth\")\n",
    "\t\t\t\tsave_checkpoint(model_engine, optimizer_deepspeed, global_step, saved_model_name, global_epoch)\n",
    "\n",
    "\t\t\t#if global_step % hparams.syncnet_eval_interval == 0:\n",
    "\t\t\t#\twith torch.no_grad():\n",
    "\t\t\t#\t\teval_loss = eval_model(test_data_loader, device, model_engine)\n",
    "\t\t\t#\t\tif eval_loss < best_loss:\n",
    "\t\t\t#\t\t\tsaved_model_name = join(checkpoint_dir, f\"best.pth\")\n",
    "\t\t\t#\t\t\tsave_checkpoint(model_engine, optimizer_deepspeed, global_step, saved_model_name, global_epoch)\n",
    "\t\t\t#\t\t\tbest_loss = eval_loss\n",
    "\t\t\t# prog_bar.set_description('Loss: {}'.format(running_loss / (step + 1)))\n",
    "\t\t\tprint(f\"Step {global_step} | Loss: {running_loss / (step + 1):.8f} | Elapsed: {(time() - st):.5f}\")\n",
    "\n",
    "\t\tglobal_epoch += 1\n",
    "\n",
    "\n",
    "def eval_model(test_data_loader, device, model):\n",
    "\teval_steps = 1400\n",
    "\tprint('Evaluating for {} steps'.format(eval_steps))\n",
    "\tlosses = []\n",
    "\twhile 1:\n",
    "\t\tfor step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\t\t\tmodel.eval()\n",
    "\n",
    "\t\t\t# Transform data to CUDA device\n",
    "\t\t\tx = x.to(device)\n",
    "\t\t\tmel = mel.to(device)\n",
    "\n",
    "\t\t\ta, v = model(mel, x)\n",
    "\t\t\ty = y.to(device)\n",
    "\n",
    "\t\t\tloss = cosine_loss(a, v, y)\n",
    "\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t\tif step > eval_steps:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\taveraged_loss = sum(losses) / len(losses)\n",
    "\t\tprint(f\"eval loss: {averaged_loss}\")\n",
    "\n",
    "\t\treturn averaged_loss\n",
    "\n",
    "\n",
    "# def save_checkpoint(model_engine, optimizer, step, checkpoint_dir, epoch):\n",
    "# \t# 获取当前时间戳\n",
    "# \tmodel_engine.save_checkpoint(checkpoint_dir)\n",
    "# \t# torch_model = model_engine.module\n",
    "# \t# output_model_path = checkpoint_dir+str(timestamp)+\"_\"+str(step)+\"_trained_syncnet.pth\"\n",
    "# \t# torch.save(torch_model.state_dict(), output_model_path)\n",
    "# \tprint(\"Saved checkpoint:\", checkpoint_dir)\n",
    "\n",
    "\n",
    "def save_checkpoint(model_engine, optimizer, step, checkpoint_path, epoch):\n",
    "\t# checkpoint_path = join(\tcheckpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
    "\toptimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "\ttorch.save({\n",
    "\t\t\"state_dict\": model_engine.state_dict(),\n",
    "\t\t\"optimizer\": optimizer_state,\n",
    "\t\t\"global_step\": step,\n",
    "\t\t\"global_epoch\": epoch,\n",
    "\t\t\"best_loss\": best_loss,\n",
    "\t}, checkpoint_path)\n",
    "\tprint(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "\tif use_cuda:\n",
    "\t\tcheckpoint = torch.load(checkpoint_path)\n",
    "\telse:\n",
    "\t\tcheckpoint = torch.load(checkpoint_path,\n",
    "\t\t                        map_location=lambda storage, loc: storage)\n",
    "\treturn checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "\tglobal global_step\n",
    "\tglobal global_epoch\n",
    "\n",
    "\tprint(\"Load checkpoint from: {}\".format(path))\n",
    "\tcheckpoint = _load(path)\n",
    "\tmodel.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\tif not reset_optimizer:\n",
    "\t\toptimizer_state = checkpoint[\"optimizer\"]\n",
    "\t\tif optimizer_state is not None:\n",
    "\t\t\tprint(\"Load optimizer state from {}\".format(path))\n",
    "\t\t\toptimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\tglobal_step = checkpoint[\"global_step\"]\n",
    "\tglobal_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tcheckpoint_dir = args.checkpoint_dir\n",
    "\tcheckpoint_path = args.checkpoint_path\n",
    "\n",
    "\tif not os.path.exists(checkpoint_dir):\n",
    "\t\tos.mkdir(checkpoint_dir)\n",
    "\n",
    "\n",
    "\tdevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\t# Model\n",
    "\tmodel = SyncNet().to(device)\n",
    "\tprint('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\toptimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=hparams.syncnet_lr)\n",
    "\n",
    "\t# deepspeed 改造\n",
    "\tmodel_engine, optimizer_deepspeed, _, _ = deepspeed.initialize(args=args, model=model, optimizer=optimizer)\n",
    "    \n",
    "    # Dataset and Dataloader setup\n",
    "\ttrain_dataset = Dataset('train')\n",
    "\ttest_dataset = Dataset('val')\n",
    "\tsampler = data_utils.DistributedSampler(train_dataset, shuffle=True)\n",
    "\ttrain_data_loader = data_utils.DataLoader(\n",
    "\t\ttrain_dataset, batch_size=hparams.syncnet_batch_size, \n",
    "\t\tnum_workers=hparams.num_workers, sampler=sampler)\n",
    "\n",
    "\ttest_data_loader = data_utils.DataLoader(\n",
    "\t\ttest_dataset, batch_size=hparams.syncnet_batch_size,\n",
    "\t\tnum_workers=hparams.num_workers,sampler=sampler)\n",
    "\n",
    "\tprint('train_dataset:', len(train_dataset))\n",
    "\tprint('test_dataset:', len(test_dataset))\n",
    "    \n",
    "    \n",
    "\tif checkpoint_path is not None:\n",
    "\t\tload_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "\n",
    "\ttrain(\n",
    "\t\tdevice, model_engine, train_data_loader, test_data_loader, optimizer_deepspeed,\n",
    "\t\tcheckpoint_dir=checkpoint_dir,\n",
    "\t\tcheckpoint_interval=hparams.syncnet_checkpoint_interval,\n",
    "\t\tnepochs=hparams.nepochs,\n",
    "\t\tsampler=sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcec451-f2d6-4069-9d28-ce1bf9c2aa78",
   "metadata": {
    "tags": []
   },
   "source": [
    "### wav2lip主模型 deepspeed 改造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea58ef0-542a-4a5d-82fa-83c5ed0b3c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/hq_wav2lip_train_deepspeed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/hq_wav2lip_train_deepspeed.py\n",
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "from models import SyncNet_color as SyncNet\n",
    "from models import Wav2Lip, Wav2Lip_disc_qual\n",
    "import audio\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import deepspeed\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Code to train the Wav2Lip model WITH the visual quality discriminator')\n",
    "\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the preprocessed LRS2 dataset\", required=True, type=str)\n",
    "\n",
    "parser.add_argument('--checkpoint_dir', help='Save checkpoints to this directory', required=True, type=str)\n",
    "parser.add_argument('--syncnet_checkpoint_path', help='Load the pre-trained Expert discriminator', required=True, type=str)\n",
    "\n",
    "parser.add_argument('--checkpoint_path', help='Resume generator from this checkpoint', default=None, type=str)\n",
    "parser.add_argument('--disc_checkpoint_path', help='Resume quality disc from this checkpoint', default=None, type=str)\n",
    "\n",
    "parser = deepspeed.add_config_arguments(parser)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(args.data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def read_window(self, window_fnames):\n",
    "        if window_fnames is None: return None\n",
    "        window = []\n",
    "        for fname in window_fnames:\n",
    "            img = cv2.imread(fname)\n",
    "            if img is None:\n",
    "                return None\n",
    "            try:\n",
    "                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "            window.append(img)\n",
    "\n",
    "        return window\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        if type(start_frame) == int:\n",
    "            start_frame_num = start_frame\n",
    "        else:\n",
    "            start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "        \n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "    def get_segmented_mels(self, spec, start_frame):\n",
    "        mels = []\n",
    "        assert syncnet_T == 5\n",
    "        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n",
    "        if start_frame_num - 2 < 0: return None\n",
    "        for i in range(start_frame_num, start_frame_num + syncnet_T):\n",
    "            m = self.crop_audio_window(spec, i - 2)\n",
    "            if m.shape[0] != syncnet_mel_step_size:\n",
    "                return None\n",
    "            mels.append(m.T)\n",
    "\n",
    "        mels = np.asarray(mels)\n",
    "\n",
    "        return mels\n",
    "\n",
    "    def prepare_window(self, window):\n",
    "        # 3 x T x H x W\n",
    "        x = np.asarray(window) / 255.\n",
    "        x = np.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            \n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            window_fnames = self.get_window(img_name)\n",
    "            wrong_window_fnames = self.get_window(wrong_img_name)\n",
    "            if window_fnames is None or wrong_window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = self.read_window(window_fnames)\n",
    "            if window is None:\n",
    "                continue\n",
    "\n",
    "            wrong_window = self.read_window(wrong_window_fnames)\n",
    "            if wrong_window is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "            \n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n",
    "            if indiv_mels is None: continue\n",
    "\n",
    "            window = self.prepare_window(window)\n",
    "            y = window.copy()\n",
    "            window[:, :, window.shape[2]//2:] = 0.\n",
    "\n",
    "            wrong_window = self.prepare_window(wrong_window)\n",
    "            x = np.concatenate([window, wrong_window], axis=0)\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n",
    "            y = torch.FloatTensor(y)\n",
    "            return x, indiv_mels, mel, y\n",
    "\n",
    "def save_sample_images(x, g, gt, global_step, checkpoint_dir):\n",
    "    x = (x.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255.).astype(np.uint8)\n",
    "    g = (g.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255.).astype(np.uint8)\n",
    "    gt = (gt.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255.).astype(np.uint8)\n",
    "\n",
    "    refs, inps = x[..., 3:], x[..., :3]\n",
    "    folder = join(checkpoint_dir, \"samples_step{:09d}\".format(global_step))\n",
    "    if not os.path.exists(folder): os.mkdir(folder)\n",
    "    collage = np.concatenate((refs, inps, g, gt), axis=-2)\n",
    "    for batch_idx, c in enumerate(collage):\n",
    "        for t in range(len(c)):\n",
    "            cv2.imwrite('{}/{}_{}.jpg'.format(folder, batch_idx, t), c[t])\n",
    "\n",
    "logloss = nn.BCELoss()\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "syncnet = SyncNet().to(device)\n",
    "for p in syncnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "recon_loss = nn.L1Loss()\n",
    "def get_sync_loss(mel, g):\n",
    "    g = g[:, :, :, g.size(3)//2:]\n",
    "    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n",
    "    # B, 3 * T, H//2, W\n",
    "    a, v = syncnet(mel, g)\n",
    "    y = torch.ones(g.size(0), 1).float().to(device)\n",
    "    return cosine_loss(a, v, y)\n",
    "\n",
    "def train(device, model_engine, disc_engine, train_data_loader, test_data_loader, optimizer_deepspeed, disc_optimizer_deepspeed,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "\n",
    "    while global_epoch < nepochs:\n",
    "        sampler.set_epoch(global_epoch)\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            disc_engine.train()\n",
    "            model_engine.train()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            ### Train generator now. Remove ALL grads. \n",
    "            #optimizer.zero_grad()\n",
    "            #disc_optimizer.zero_grad()\n",
    "\n",
    "            g = model_engine(indiv_mels, x)\n",
    "\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g)\n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            ###deepspeed改造####\n",
    "            model_engine.backward(loss)\n",
    "            model_engine.step()\n",
    "            #loss.backward()\n",
    "            #optimizer.step()\n",
    "\n",
    "            ### Remove all gradients before Training disc\n",
    "            #disc_optimizer.zero_grad()\n",
    "\n",
    "            pred = disc_engine(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "            #disc_real_loss.backward()\n",
    "            disc_engine.backward(disc_real_loss)\n",
    "\n",
    "            pred = disc_engine(g.detach())\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "            disc_engine.backward(disc_fake_loss)\n",
    "            disc_engine.step()\n",
    "            #disc_fake_loss.backward()\n",
    "\n",
    "            #disc_optimizer.step()\n",
    "\n",
    "            running_disc_real_loss += disc_real_loss.item()\n",
    "            running_disc_fake_loss += disc_fake_loss.item()\n",
    "\n",
    "            if global_step % checkpoint_interval == 0:\n",
    "                save_sample_images(x, g, gt, global_step, checkpoint_dir)\n",
    "\n",
    "            # Logs\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "\n",
    "            running_l1_loss += l1loss.item()\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                running_sync_loss += sync_loss.item()\n",
    "            else:\n",
    "                running_sync_loss += 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss += perceptual_loss.item()\n",
    "            else:\n",
    "                running_perceptual_loss += 0.\n",
    "\n",
    "            if global_step==1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "                save_checkpoint(disc_engine, disc_optimizer_deepspeed, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n",
    "\n",
    "\n",
    "            #if global_step % hparams.eval_interval == 0:\n",
    "            #    with torch.no_grad():\n",
    "            #        average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n",
    "\n",
    "            #        if average_sync_loss < .75:\n",
    "            #            hparams.set_hparam('syncnet_wt', 0.03)\n",
    "\n",
    "            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n",
    "                                                                                        running_sync_loss / (step + 1),\n",
    "                                                                                        running_perceptual_loss / (step + 1),\n",
    "                                                                                        running_disc_fake_loss / (step + 1),\n",
    "                                                                                        running_disc_real_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, disc):\n",
    "    eval_steps = 300\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n",
    "    while 1:\n",
    "        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n",
    "            model.eval()\n",
    "            disc.eval()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "            pred = disc(g)\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "\n",
    "            running_disc_real_loss.append(disc_real_loss.item())\n",
    "            running_disc_fake_loss.append(disc_fake_loss.item())\n",
    "\n",
    "            sync_loss = get_sync_loss(mel, g)\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            running_l1_loss.append(l1loss.item())\n",
    "            running_sync_loss.append(sync_loss.item())\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss.append(perceptual_loss.item())\n",
    "            else:\n",
    "                running_perceptual_loss.append(0.)\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n",
    "                                                            sum(running_sync_loss) / len(running_sync_loss),\n",
    "                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n",
    "                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n",
    "                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n",
    "        return sum(running_sync_loss) / len(running_sync_loss)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint_syncnet(path, model):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"module\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "     # Model\n",
    "    model = Wav2Lip().to(device)\n",
    "    disc = Wav2Lip_disc_qual().to(device)\n",
    "\n",
    "    print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "    print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n",
    "\n",
    "    optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=hparams.initial_learning_rate, betas=(0.5, 0.999))\n",
    "    disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n",
    "                           lr=hparams.disc_initial_learning_rate, betas=(0.5, 0.999))\n",
    "    \n",
    "    \n",
    "\n",
    "    if args.checkpoint_path is not None:\n",
    "        load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "\n",
    "    if args.disc_checkpoint_path is not None:\n",
    "        load_checkpoint(args.disc_checkpoint_path, disc, disc_optimizer, \n",
    "                                reset_optimizer=False, overwrite_global_states=False)\n",
    "       # deepspeed 改造\n",
    "    model_engine, optimizer_deepspeed, _, _ = deepspeed.initialize(args=args, model=model, optimizer=optimizer)\n",
    "    model_disc_engine, optimizer_disc_deepspeed, _, _ = deepspeed.initialize(args=args, model=disc, optimizer=disc_optimizer)\n",
    "    \n",
    "        # Dataset and Dataloader setup\n",
    "    train_dataset = Dataset('train')\n",
    "    test_dataset = Dataset('val')\n",
    "\n",
    "    sampler = data_utils.DistributedSampler(train_dataset, shuffle=True)\n",
    "    train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "        num_workers=hparams.num_workers,sampler=sampler)\n",
    "\n",
    "    test_data_loader = data_utils.DataLoader(\n",
    "        test_dataset, batch_size=hparams.batch_size,\n",
    "        num_workers=hparams.num_workers,sampler=sampler\n",
    "    \n",
    "    \n",
    "    load_checkpoint_syncnet(args.syncnet_checkpoint_path, syncnet)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    # Train!\n",
    "    train(device, model_engine, model_disc_engine, train_data_loader, test_data_loader, optimizer_deepspeed, optimizer_disc_deepspeed,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              checkpoint_interval=hparams.checkpoint_interval,\n",
    "              nepochs=hparams.nepochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183cbc0-32c4-4016-a3b6-b19d627241a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b9cfe0-ad7c-4fe7-8e81-3a0389d2cc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/ds_config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/ds_config.json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": false,\n",
    "    \"auto_cast\": true,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"betas\": [\n",
    "        0.8,\n",
    "        0.999\n",
    "      ],\n",
    "      \"eps\": 1e-8,\n",
    "      \"weight_decay\": 3e-7\n",
    "    }\n",
    "  },\n",
    "   \"scheduler\": {\n",
    "    \"type\": \"OneCycle\",\n",
    "    \"params\": {\n",
    "        \"cycle_min_lr\": 1e-04,\n",
    "        \"cycle_max_lr\": 3e-04\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "      \"stage\": 0\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": 2,\n",
    "  \"gradient_clipping\": 0.0,\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": 64,\n",
    "  \"train_micro_batch_size_per_gpu\": 4,\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3436f11c-46b2-4586-8fec-bed0e7b37d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./wav2lip_288x288/train-distribute.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./wav2lip_288x288/train-distribute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://${MODEL_S3_BUCKET}/wav2lip_288x288/train_data/main2/* /tmp/main2/\n",
    "rm -rf /tmp/main2/5540197286159433545/.ipynb_checkpoints\n",
    "rm -rf /tmp/main2/5536876846942893978/.ipynb_checkpoints\n",
    "pip install -r ./requirements.txt\n",
    "\n",
    "echo \"--------preprocess wav-----------\"\n",
    "python ./preprocess.py --data_root /tmp/main2/ --preprocessed_root /tmp/lrs2_preprocessed2/\n",
    "echo \"finished preprocess\"\n",
    "\n",
    "echo \"--------prepare tain list--------\"\n",
    "python ./generate_filelists.py --data_root /tmp/lrs2_preprocessed2/\n",
    "cat filelists/train.txt\n",
    "echo \"finished train list prepare\"\n",
    "\n",
    "echo \"--------train the expert discriminator------\"\n",
    "startdt=$(date +%s)\n",
    "deepspeed --num_gpus=8 ./color_syncnet_dist_train.py \\\n",
    "          --data_root /tmp/lrs2_preprocessed2/ \\\n",
    "          --checkpoint_dir /tmp/trained_syncnet/  \\\n",
    "          --deepspeed --deepspeed_config ds_config.json\n",
    "enddt=$(date +%s)\n",
    "interval_minutes=$(( (enddt - startdt) ))\n",
    "echo \"时间间隔为 $interval_minutes 秒\"\n",
    "echo \"finished train syncnet\"\n",
    "#\n",
    "#trained_syncnet_model_file=$(find /tmp/trained_syncnet/ -name  \"*.pt\" -print)\n",
    "#trained_syncnet_model_file=$(echo $trained_syncnet_model_file|cut -d' ' -f1)\n",
    "#echo \"trained_syncnet_model_file: \"${trained_syncnet_model_file}\n",
    "    \n",
    "#echo \"--------train wav2lip-----------------------\" \n",
    "#python ./hq_wav2lip_train_deepspeed.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_wav2lip_288x288/ --syncnet_checkpoint_path $trained_syncnet_model_file\n",
    "#python ./hq_wav2lip_train.py --data_root /tmp/lrs2_preprocessed2/ --checkpoint_dir /tmp/trained_wav2lip_288x288/ --syncnet_checkpoint_path /tmp/trained_syncnet/10/global_step2/mp_rank_00_model_states.pt\n",
    "          \n",
    "#echo \"finished wav2lip\"\n",
    "\n",
    "#./s5cmd sync /tmp/trained_wav2lip_288x288/ s3://$MODEL_S3_BUCKET/wav2lip_288x288/output/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "\n",
    "###inference\n",
    "#echo \"begin inference\"\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/face_video/* /tmp/face_video/\n",
    "#./s5cmd sync  s3://${sagemaker_default_bucket}/wav2lip/inference/audio/* /tmp/audio/\n",
    "#python ./inference.py --checkpoint_path /tmp/trained_wav2lip_288x288/checkpoint_step000000001.pth  --face /tmp/face_video/VID20230623143819.mp4 --audio /tmp/audio/测试wav2lip.mp3\n",
    "#./s5cmd sync ./results/result_voice.mp4  s3://${sagemaker_default_bucket}/models/wav2lip_288x288/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372dad3-4388-4925-91c4-a44b2f95bca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'wav2lip-288x288-demo'         \n",
    "\n",
    "instance_type = 'ml.g5.48xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train-distribute.sh',\n",
    "                      source_dir='./wav2lip_288x288/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      keep_alive_period_in_seconds=3600,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e673c",
   "metadata": {
    "tags": []
   },
   "source": [
    "You could find the model path in S3 from above logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54873d2f-2b86-41f9-ae2d-f71693d92cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
